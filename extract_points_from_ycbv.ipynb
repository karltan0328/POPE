{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_name: ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross']\n",
      "layer_name: ['self', 'cross']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-04 12:58:52.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpope_model_api\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mload Matcher successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pope_model_api import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-04 12:58:59.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mload SAM model from weights/sam_vit_h_4b8939.pth\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt, model_type = get_model_info(\"h\")\n",
    "sam = sam_model_registry[model_type](checkpoint=ckpt)\n",
    "DEVICE = \"cuda\"\n",
    "sam.to(device=DEVICE)\n",
    "MASK_GEN = SamAutomaticMaskGenerator(sam)\n",
    "logger.info(f\"load SAM model from {ckpt}\")\n",
    "crop_tool = CropImage()\n",
    "dinov2_model = load_dinov2_model()\n",
    "dinov2_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "metrics.update({'R_errs': [], 't_errs': [], 'inliers': [] , \"identifiers\":[] })\n",
    "\n",
    "\n",
    "# load data ROOR_DIR\n",
    "ROOT_DIR = \"data/ycbv/\"\n",
    "res_table = []\n",
    "\n",
    "import json\n",
    "with open(\"data/pairs/YCB-VIDEO-test.json\") as f:\n",
    "    dir_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-04 12:59:00.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 0\u001b[0m\n",
      "  0%|          | 0/108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [01:54<00:00,  1.06s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.09s/it]\n",
      "100%|██████████| 66/66 [01:14<00:00,  1.13s/it]\n",
      "100%|██████████| 64/64 [01:01<00:00,  1.05it/s]\n",
      "100%|██████████| 39/39 [00:42<00:00,  1.09s/it]\n",
      "100%|██████████| 114/114 [01:48<00:00,  1.05it/s]\n",
      "\u001b[32m2024-04-04 13:06:22.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 1\u001b[0m\n",
      "100%|██████████| 77/77 [02:26<00:00,  1.90s/it]\n",
      "100%|██████████| 33/33 [01:01<00:00,  1.86s/it]\n",
      "100%|██████████| 27/27 [00:46<00:00,  1.72s/it]\n",
      "100%|██████████| 52/52 [01:43<00:00,  1.98s/it]\n",
      "100%|██████████| 43/43 [01:26<00:00,  2.01s/it]\n",
      "100%|██████████| 15/15 [00:29<00:00,  1.98s/it]\n",
      "\u001b[32m2024-04-04 13:14:16.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 2\u001b[0m\n",
      "100%|██████████| 52/52 [00:54<00:00,  1.05s/it]\n",
      "100%|██████████| 84/84 [01:49<00:00,  1.30s/it]\n",
      "100%|██████████| 91/91 [01:49<00:00,  1.21s/it]\n",
      "100%|██████████| 73/73 [01:20<00:00,  1.11s/it]\n",
      "100%|██████████| 101/101 [02:09<00:00,  1.28s/it]\n",
      "100%|██████████| 94/94 [01:50<00:00,  1.18s/it]\n",
      "\u001b[32m2024-04-04 13:24:11.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 3\u001b[0m\n",
      "100%|██████████| 67/67 [01:05<00:00,  1.03it/s]\n",
      "100%|██████████| 56/56 [00:56<00:00,  1.02s/it]\n",
      "100%|██████████| 73/73 [01:13<00:00,  1.01s/it]\n",
      "100%|██████████| 56/56 [01:03<00:00,  1.14s/it]\n",
      "100%|██████████| 52/52 [00:53<00:00,  1.03s/it]\n",
      "100%|██████████| 43/43 [00:39<00:00,  1.10it/s]\n",
      "\u001b[32m2024-04-04 13:30:03.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 4\u001b[0m\n",
      "100%|██████████| 97/97 [02:17<00:00,  1.41s/it]\n",
      "100%|██████████| 55/55 [01:11<00:00,  1.29s/it]\n",
      "100%|██████████| 25/25 [00:26<00:00,  1.07s/it]\n",
      "100%|██████████| 47/47 [01:10<00:00,  1.49s/it]\n",
      "100%|██████████| 28/28 [00:44<00:00,  1.58s/it]\n",
      "100%|██████████| 28/28 [00:35<00:00,  1.27s/it]\n",
      "\u001b[32m2024-04-04 13:36:28.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 5\u001b[0m\n",
      "100%|██████████| 59/59 [00:52<00:00,  1.13it/s]\n",
      "100%|██████████| 24/24 [00:22<00:00,  1.08it/s]\n",
      "100%|██████████| 59/59 [00:50<00:00,  1.17it/s]\n",
      "100%|██████████| 44/44 [00:40<00:00,  1.08it/s]\n",
      "100%|██████████| 51/51 [00:46<00:00,  1.10it/s]\n",
      "100%|██████████| 47/47 [00:42<00:00,  1.10it/s]\n",
      "\u001b[32m2024-04-04 13:40:42.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 6\u001b[0m\n",
      "100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "\u001b[32m2024-04-04 13:41:27.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 7\u001b[0m\n",
      "100%|██████████| 64/64 [01:21<00:00,  1.28s/it]\n",
      "100%|██████████| 28/28 [00:36<00:00,  1.32s/it]\n",
      "100%|██████████| 25/25 [00:31<00:00,  1.26s/it]\n",
      "100%|██████████| 9/9 [00:11<00:00,  1.22s/it]\n",
      "100%|██████████| 49/49 [01:02<00:00,  1.28s/it]\n",
      "100%|██████████| 35/35 [00:48<00:00,  1.38s/it]\n",
      "\u001b[32m2024-04-04 13:45:59.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 8\u001b[0m\n",
      "100%|██████████| 88/88 [01:35<00:00,  1.08s/it]\n",
      "100%|██████████| 33/33 [00:35<00:00,  1.06s/it]\n",
      "100%|██████████| 13/13 [00:14<00:00,  1.11s/it]\n",
      "100%|██████████| 7/7 [00:05<00:00,  1.38it/s]\n",
      "100%|██████████| 34/34 [00:33<00:00,  1.01it/s]\n",
      "100%|██████████| 66/66 [01:23<00:00,  1.26s/it]\n",
      "\u001b[32m2024-04-04 13:50:25.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mYCBVIDEO: 9\u001b[0m\n",
      "100%|██████████| 94/94 [02:02<00:00,  1.30s/it]\n",
      "100%|██████████| 36/36 [00:58<00:00,  1.63s/it]\n",
      "100%|██████████| 26/26 [00:46<00:00,  1.77s/it]\n",
      "100%|██████████| 39/39 [01:04<00:00,  1.66s/it]\n",
      "100%|██████████| 36/36 [00:39<00:00,  1.10s/it]\n",
      "100%|██████████| 47/47 [00:50<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "for label_idx, test_dict in enumerate(dir_list):\n",
    "    logger.info(f\"YCBVIDEO: {label_idx}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    sample_data = dir_list[label_idx][\"0\"][0]\n",
    "    label = sample_data.split(\"/\")[0]\n",
    "    name = label.split(\"-\")[1]\n",
    "    dir_name = os.path.dirname(sample_data)\n",
    "    FULL_ROOT_DIR = os.path.join(ROOT_DIR, dir_name)\n",
    "    recall_image, all_image = 0, 0\n",
    "    for rotation_key, rotation_list in zip(test_dict.keys(), test_dict.values()):\n",
    "        for pair_idx, pair_name in enumerate(tqdm(rotation_list)):\n",
    "            all_image = all_image + 1\n",
    "            base_name = os.path.basename(pair_name)\n",
    "            idx0_name = base_name.split(\"png-\")[0] + \"png\"\n",
    "            idx1_name = base_name.split(\"png-\")[1]\n",
    "            image0_name = os.path.join(FULL_ROOT_DIR, idx0_name)\n",
    "            image1_name = os.path.join(FULL_ROOT_DIR.replace(\"color\", \"color_full\"), idx1_name)\n",
    "            intrinsic_path = image0_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "\n",
    "            try:\n",
    "                K0 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "            except:\n",
    "                # print(f'{intrinsic_path} is not exist')\n",
    "                continue\n",
    "            intrinsic_path = image1_name.replace(\"color_full\", \"intrin\").replace(\"png\", \"txt\")\n",
    "            try:\n",
    "                K1 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "            except:\n",
    "                # print(f'{intrinsic_path} is not exist')\n",
    "                continue\n",
    "            image0 = cv2.imread(image0_name)\n",
    "            ref_torch_image = set_torch_image(image0, center_crop=True)\n",
    "            ref_fea = get_cls_token_torch(dinov2_model, ref_torch_image)\n",
    "            image1 = cv2.imread(image1_name)\n",
    "            image_h, image_w, _ = image1.shape\n",
    "            t1 = time.time()\n",
    "            masks = MASK_GEN.generate(image1)\n",
    "            t2 = time.time()\n",
    "            similarity_score, top_images = np.array([0, 0, 0], np.float32), [[], [], []]\n",
    "            t3 = time.time()\n",
    "            compact_percent = 0.3\n",
    "            for xxx, mask in enumerate(masks):\n",
    "                object_mask = np.expand_dims(mask[\"segmentation\"], -1)\n",
    "                x0, y0, w, h = mask[\"bbox\"]\n",
    "                x1, y1 = x0 + w, y0 + h\n",
    "                x0 -= int(w * compact_percent)\n",
    "                y0 -= int(h * compact_percent)\n",
    "                x1 += int(w * compact_percent)\n",
    "                y1 += int(h * compact_percent)\n",
    "                box = np.array([x0, y0, x1, y1])\n",
    "                resize_shape = np.array([y1 - y0, x1 - x0])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box, K1, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image1, box, resize_shape)\n",
    "                # object_mask,_ = get_image_crop_resize(object_mask, box, resize_shape)\n",
    "                box_new = np.array([0, 0, x1 - x0, y1 - y0])\n",
    "                resize_shape = np.array([256, 256])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box_new, K_crop, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image_crop, box_new, resize_shape)\n",
    "                crop_tensor = set_torch_image(image_crop, center_crop=True)\n",
    "                with torch.no_grad():\n",
    "                    fea = get_cls_token_torch(dinov2_model, crop_tensor)\n",
    "                score = F.cosine_similarity(ref_fea, fea, dim=1, eps=1e-8)\n",
    "                if (score.item() > similarity_score).any():\n",
    "                    mask[\"crop_image\"] = image_crop\n",
    "                    mask[\"K\"] = K_crop\n",
    "                    mask[\"bbox\"] = box\n",
    "                    min_idx = np.argmin(similarity_score)\n",
    "                    similarity_score[min_idx] = score.item()\n",
    "                    top_images[min_idx] = mask.copy()\n",
    "\n",
    "            crop_img0 = image0\n",
    "            img0 = cv2.cvtColor(image0, cv2.COLOR_BGR2GRAY)\n",
    "            img0 = torch.from_numpy(img0).float()[None] / 255.\n",
    "            img0 = img0.unsqueeze(0).cuda()\n",
    "\n",
    "            matching_score = [[0] for _ in range(len(top_images))]\n",
    "            for top_idx in range(len(top_images)):\n",
    "                crop_img1 = top_images[top_idx][\"crop_image\"]\n",
    "                img1 = cv2.cvtColor(top_images[top_idx][\"crop_image\"], cv2.COLOR_BGR2GRAY)\n",
    "                img1 = torch.from_numpy(img1).float()[None] / 255.\n",
    "                img1 = img1.unsqueeze(0).cuda()\n",
    "                batch = {'image0':img0, 'image1':img1}\n",
    "                with torch.no_grad():\n",
    "                    matcher(batch)\n",
    "                    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "                    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "                    confidences = batch[\"mconf\"].cpu().numpy()\n",
    "                conf_mask = np.where(confidences > 0.9)\n",
    "                matching_score[top_idx] = conf_mask[0].shape[0]\n",
    "                top_images[top_idx][\"mkpts0\"] = mkpts0\n",
    "                top_images[top_idx][\"mkpts1\"] = mkpts1\n",
    "                top_images[top_idx][\"mconf\"] = confidences\n",
    "                top_images[top_idx][\"crop_img0\"] = crop_img0\n",
    "                top_images[top_idx][\"crop_img1\"] = crop_img1\n",
    "            #---------------------------------------------------\n",
    "            # crop_image = cv2.resize(top_images[np.argmax(matching_score)][\"crop_image\"],(256,256))\n",
    "            # que_image = cv2.resize(image0,(256,256))\n",
    "            # image = np.hstack((que_image, crop_image))\n",
    "            # for top_idx in range(len(top_images)):\n",
    "            #     crop_image = top_images[top_idx][\"crop_image\"]\n",
    "            #     score = matching_score[top_idx]\n",
    "            #     crop_image = cv2.resize(crop_image,(256,256))\n",
    "            #     cv2.putText(crop_image,f'{score}',(100,100),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),1)\n",
    "            #     image = np.hstack((image, crop_image))\n",
    "            # cv2.imwrite(f\"segment_anything/crop_images/{idx}.jpg\", image)\n",
    "            #---------------------------------------------------\n",
    "            t4 = time.time()\n",
    "            # print(f\"t4-t3: object detection:{1000*(t4-t3)} ms\")\n",
    "            pose0_name = image0_name.replace(\"color\", \"poses_ba\").replace(\"png\", \"txt\")\n",
    "            pose1_name = image1_name.replace(\"color_full\", \"poses_ba\").replace(\"png\", \"txt\")\n",
    "            pose0 = np.loadtxt(pose0_name)\n",
    "            pose1 = np.loadtxt(pose1_name)\n",
    "            pose0 = np.vstack((pose0, np.array([[0, 0, 0, 1]])))\n",
    "            pose1 = np.vstack((pose1, np.array([[0, 0, 0, 1]])))\n",
    "            relative_pose =  np.matmul(pose1, inv(pose0))\n",
    "            t = relative_pose[:3, -1].reshape(1, 3)\n",
    "\n",
    "            max_match_idx = np.argmax(matching_score)\n",
    "            pre_bbox  = top_images[max_match_idx][\"bbox\"]\n",
    "            mkpts0 = top_images[max_match_idx][\"mkpts0\"]\n",
    "            mkpts1 = top_images[max_match_idx][\"mkpts1\"]\n",
    "            pre_K = top_images[max_match_idx][\"K\"]\n",
    "            crop_img0 = top_images[max_match_idx][\"crop_img0\"]\n",
    "            crop_img1 = top_images[max_match_idx][\"crop_img1\"]\n",
    "            if (mkpts0.shape[0] < 5\n",
    "                or mkpts1.shape[0] < 5\n",
    "                or pre_K.shape[0] != 3):\n",
    "                continue\n",
    "\n",
    "            points_file_path = os.path.join(\"data/ycbv-points/\", pair_name.split(\"/\")[0])\n",
    "            pre_bbox_path = os.path.join(points_file_path, \"pre_bbox\")\n",
    "            mkpts0_path = os.path.join(points_file_path, \"mkpts0\")\n",
    "            mkpts1_path = os.path.join(points_file_path, \"mkpts1\")\n",
    "            pre_K_path = os.path.join(points_file_path, \"pre_K\")\n",
    "            crop_img0_path = os.path.join(points_file_path, \"img0\")\n",
    "            crop_img1_path = os.path.join(points_file_path, \"img1\")\n",
    "            Path(pre_bbox_path).mkdir(parents=True, exist_ok=True)\n",
    "            Path(mkpts0_path).mkdir(parents=True, exist_ok=True)\n",
    "            Path(mkpts1_path).mkdir(parents=True, exist_ok=True)\n",
    "            Path(pre_K_path).mkdir(parents=True, exist_ok=True)\n",
    "            Path(crop_img0_path).mkdir(parents=True, exist_ok=True)\n",
    "            Path(crop_img1_path).mkdir(parents=True, exist_ok=True)\n",
    "            # print(\"points_file_path =\", points_file_path)\n",
    "            # print(\"mkpts0_path =\", mkpts0_path)\n",
    "            # print(\"mkpts1_path =\", mkpts1_path)\n",
    "            # print(\"pre_K_path =\", pre_K_path)\n",
    "            points_name = pair_name.split(\"/\")[-1]\n",
    "\n",
    "            np.savetxt(os.path.join(pre_bbox_path, f\"{points_name}.txt\"), pre_bbox)\n",
    "            np.savetxt(os.path.join(mkpts0_path, f\"{points_name}.txt\"), mkpts0)\n",
    "            np.savetxt(os.path.join(mkpts1_path, f\"{points_name}.txt\"), mkpts1)\n",
    "            np.savetxt(os.path.join(pre_K_path, f\"{points_name}.txt\"), pre_K)\n",
    "            cv2.imwrite(os.path.join(crop_img0_path, f\"{points_name}.png\"), crop_img0)\n",
    "            cv2.imwrite(os.path.join(crop_img1_path, f\"{points_name}.png\"), crop_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
