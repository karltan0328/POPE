{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_name: ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross']\n",
      "layer_name: ['self', 'cross']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-19 14:15:39.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpope_model_api\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mload Matcher successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pope_model_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-19 14:15:47.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mload SAM model from weights/sam_vit_h_4b8939.pth\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ckpt, model_type = get_model_info(\"h\")\n",
    "sam = sam_model_registry[model_type](checkpoint=ckpt)\n",
    "DEVICE = \"cuda\"\n",
    "sam.to(device=DEVICE)\n",
    "MASK_GEN = SamAutomaticMaskGenerator(sam)\n",
    "logger.info(f\"load SAM model from {ckpt}\")\n",
    "crop_tool = CropImage()\n",
    "dinov2_model = load_dinov2_model()\n",
    "dinov2_model.to(\"cuda:0\")\n",
    "metrics = dict()\n",
    "metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "ROOT_DIR = \"data/LM_dataset/\"\n",
    "dir_list = os.listdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name_dict = {\n",
    "    1: \"ape\",\n",
    "    2: \"benchvise\",\n",
    "    4: \"camera\",\n",
    "    5: \"can\",\n",
    "    6: \"cat\",\n",
    "    8: \"driller\",\n",
    "    9: \"duck\",\n",
    "    10: \"eggbox\",\n",
    "    11: \"glue\",\n",
    "    12: \"holepuncher\",\n",
    "    13: \"iron\",\n",
    "    14: \"lamp\",\n",
    "    15: \"phone\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "res_table = []\n",
    "\n",
    "import json\n",
    "with open(\"data/pairs/LINEMOD-test.json\") as f:\n",
    "    dir_list = json.load(f)\n",
    "len(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-19 14:15:49.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mLINEMOD: 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_idx = 0\n",
      "test_dict = {'0': ['0801-lm1-others/lm1-3/color/458.png-700.png', '0801-lm1-others/lm1-3/color/611.png-608.png', '0801-lm1-others/lm1-3/color/630.png-631.png', '0801-lm1-others/lm1-3/color/389.png-332.png', '0801-lm1-others/lm1-3/color/895.png-894.png', '0801-lm1-others/lm1-3/color/970.png-867.png', '0801-lm1-others/lm1-3/color/140.png-139.png', '0801-lm1-others/lm1-3/color/469.png-544.png', '0801-lm1-others/lm1-3/color/247.png-232.png', '0801-lm1-others/lm1-3/color/935.png-770.png', '0801-lm1-others/lm1-3/color/135.png-107.png', '0801-lm1-others/lm1-3/color/201.png-200.png', '0801-lm1-others/lm1-3/color/868.png-970.png', '0801-lm1-others/lm1-3/color/713.png-627.png', '0801-lm1-others/lm1-3/color/955.png-954.png', '0801-lm1-others/lm1-3/color/119.png-114.png', '0801-lm1-others/lm1-3/color/768.png-766.png', '0801-lm1-others/lm1-3/color/196.png-195.png', '0801-lm1-others/lm1-3/color/623.png-624.png', '0801-lm1-others/lm1-3/color/877.png-1037.png', '0801-lm1-others/lm1-3/color/909.png-907.png', '0801-lm1-others/lm1-3/color/333.png-445.png', '0801-lm1-others/lm1-3/color/312.png-313.png', '0801-lm1-others/lm1-3/color/322.png-321.png', '0801-lm1-others/lm1-3/color/993.png-1028.png', '0801-lm1-others/lm1-3/color/663.png-664.png', '0801-lm1-others/lm1-3/color/974.png-973.png', '0801-lm1-others/lm1-3/color/1044.png-899.png', '0801-lm1-others/lm1-3/color/412.png-413.png', '0801-lm1-others/lm1-3/color/541.png-540.png', '0801-lm1-others/lm1-3/color/938.png-765.png', '0801-lm1-others/lm1-3/color/967.png-839.png', '0801-lm1-others/lm1-3/color/969.png-1002.png', '0801-lm1-others/lm1-3/color/10.png-9.png', '0801-lm1-others/lm1-3/color/303.png-301.png', '0801-lm1-others/lm1-3/color/119.png-215.png', '0801-lm1-others/lm1-3/color/964.png-962.png', '0801-lm1-others/lm1-3/color/338.png-339.png', '0801-lm1-others/lm1-3/color/1006.png-898.png', '0801-lm1-others/lm1-3/color/285.png-401.png', '0801-lm1-others/lm1-3/color/780.png-627.png', '0801-lm1-others/lm1-3/color/725.png-805.png', '0801-lm1-others/lm1-3/color/444.png-443.png', '0801-lm1-others/lm1-3/color/48.png-49.png', '0801-lm1-others/lm1-3/color/1030.png-1045.png', '0801-lm1-others/lm1-3/color/370.png-329.png', '0801-lm1-others/lm1-3/color/795.png-653.png', '0801-lm1-others/lm1-3/color/173.png-45.png', '0801-lm1-others/lm1-3/color/856.png-855.png', '0801-lm1-others/lm1-3/color/14.png-13.png', '0801-lm1-others/lm1-3/color/502.png-501.png', '0801-lm1-others/lm1-3/color/209.png-176.png', '0801-lm1-others/lm1-3/color/456.png-455.png', '0801-lm1-others/lm1-3/color/81.png-240.png', '0801-lm1-others/lm1-3/color/595.png-596.png', '0801-lm1-others/lm1-3/color/75.png-74.png', '0801-lm1-others/lm1-3/color/45.png-173.png', '0801-lm1-others/lm1-3/color/573.png-430.png', '0801-lm1-others/lm1-3/color/802.png-801.png', '0801-lm1-others/lm1-3/color/278.png-277.png', '0801-lm1-others/lm1-3/color/269.png-268.png', '0801-lm1-others/lm1-3/color/795.png-682.png', '0801-lm1-others/lm1-3/color/715.png-716.png', '0801-lm1-others/lm1-3/color/410.png-352.png', '0801-lm1-others/lm1-3/color/811.png-812.png', '0801-lm1-others/lm1-3/color/1013.png-913.png', '0801-lm1-others/lm1-3/color/113.png-112.png', '0801-lm1-others/lm1-3/color/134.png-133.png', '0801-lm1-others/lm1-3/color/130.png-43.png', '0801-lm1-others/lm1-3/color/497.png-431.png', '0801-lm1-others/lm1-3/color/766.png-767.png', '0801-lm1-others/lm1-3/color/801.png-802.png', '0801-lm1-others/lm1-3/color/716.png-715.png', '0801-lm1-others/lm1-3/color/669.png-605.png', '0801-lm1-others/lm1-3/color/378.png-379.png', '0801-lm1-others/lm1-3/color/202.png-201.png', '0801-lm1-others/lm1-3/color/821.png-1019.png', '0801-lm1-others/lm1-3/color/964.png-899.png', '0801-lm1-others/lm1-3/color/592.png-608.png', '0801-lm1-others/lm1-3/color/766.png-767.png'], '1': ['0801-lm1-others/lm1-3/color/708.png-744.png', '0801-lm1-others/lm1-3/color/226.png-213.png', '0801-lm1-others/lm1-3/color/714.png-783.png', '0801-lm1-others/lm1-3/color/863.png-981.png', '0801-lm1-others/lm1-3/color/820.png-923.png', '0801-lm1-others/lm1-3/color/831.png-830.png', '0801-lm1-others/lm1-3/color/935.png-939.png', '0801-lm1-others/lm1-3/color/175.png-176.png', '0801-lm1-others/lm1-3/color/721.png-628.png', '0801-lm1-others/lm1-3/color/263.png-495.png', '0801-lm1-others/lm1-3/color/193.png-85.png', '0801-lm1-others/lm1-3/color/19.png-21.png', '0801-lm1-others/lm1-3/color/662.png-667.png', '0801-lm1-others/lm1-3/color/219.png-218.png', '0801-lm1-others/lm1-3/color/761.png-765.png', '0801-lm1-others/lm1-3/color/670.png-665.png', '0801-lm1-others/lm1-3/color/759.png-757.png', '0801-lm1-others/lm1-3/color/744.png-709.png', '0801-lm1-others/lm1-3/color/578.png-332.png', '0801-lm1-others/lm1-3/color/127.png-36.png', '0801-lm1-others/lm1-3/color/721.png-779.png', '0801-lm1-others/lm1-3/color/741.png-626.png', '0801-lm1-others/lm1-3/color/770.png-684.png', '0801-lm1-others/lm1-3/color/594.png-452.png', '0801-lm1-others/lm1-3/color/785.png-748.png', '0801-lm1-others/lm1-3/color/877.png-876.png', '0801-lm1-others/lm1-3/color/748.png-749.png', '0801-lm1-others/lm1-3/color/596.png-453.png', '0801-lm1-others/lm1-3/color/243.png-30.png', '0801-lm1-others/lm1-3/color/911.png-1012.png', '0801-lm1-others/lm1-3/color/1042.png-1008.png', '0801-lm1-others/lm1-3/color/572.png-443.png', '0801-lm1-others/lm1-3/color/254.png-135.png', '0801-lm1-others/lm1-3/color/771.png-684.png', '0801-lm1-others/lm1-3/color/270.png-273.png', '0801-lm1-others/lm1-3/color/698.png-792.png', '0801-lm1-others/lm1-3/color/230.png-58.png', '0801-lm1-others/lm1-3/color/910.png-905.png', '0801-lm1-others/lm1-3/color/410.png-411.png', '0801-lm1-others/lm1-3/color/727.png-723.png', '0801-lm1-others/lm1-3/color/104.png-249.png', '0801-lm1-others/lm1-3/color/785.png-749.png', '0801-lm1-others/lm1-3/color/999.png-922.png', '0801-lm1-others/lm1-3/color/231.png-248.png', '0801-lm1-others/lm1-3/color/247.png-54.png', '0801-lm1-others/lm1-3/color/594.png-569.png', '0801-lm1-others/lm1-3/color/480.png-477.png', '0801-lm1-others/lm1-3/color/334.png-380.png', '0801-lm1-others/lm1-3/color/186.png-180.png', '0801-lm1-others/lm1-3/color/477.png-478.png', '0801-lm1-others/lm1-3/color/574.png-497.png', '0801-lm1-others/lm1-3/color/1040.png-867.png', '0801-lm1-others/lm1-3/color/19.png-17.png', '0801-lm1-others/lm1-3/color/575.png-498.png', '0801-lm1-others/lm1-3/color/757.png-759.png', '0801-lm1-others/lm1-3/color/314.png-316.png', '0801-lm1-others/lm1-3/color/76.png-72.png', '0801-lm1-others/lm1-3/color/397.png-394.png', '0801-lm1-others/lm1-3/color/25.png-26.png', '0801-lm1-others/lm1-3/color/983.png-84.png', '0801-lm1-others/lm1-3/color/595.png-454.png', '0801-lm1-others/lm1-3/color/67.png-1031.png', '0801-lm1-others/lm1-3/color/31.png-215.png', '0801-lm1-others/lm1-3/color/62.png-60.png', '0801-lm1-others/lm1-3/color/674.png-675.png', '0801-lm1-others/lm1-3/color/440.png-578.png', '0801-lm1-others/lm1-3/color/458.png-791.png', '0801-lm1-others/lm1-3/color/418.png-540.png', '0801-lm1-others/lm1-3/color/130.png-37.png', '0801-lm1-others/lm1-3/color/998.png-828.png', '0801-lm1-others/lm1-3/color/885.png-888.png', '0801-lm1-others/lm1-3/color/44.png-251.png', '0801-lm1-others/lm1-3/color/416.png-418.png', '0801-lm1-others/lm1-3/color/30.png-121.png', '0801-lm1-others/lm1-3/color/510.png-505.png', '0801-lm1-others/lm1-3/color/331.png-501.png', '0801-lm1-others/lm1-3/color/175.png-42.png', '0801-lm1-others/lm1-3/color/796.png-652.png', '0801-lm1-others/lm1-3/color/654.png-751.png', '0801-lm1-others/lm1-3/color/131.png-106.png'], '2': ['0801-lm1-others/lm1-3/color/682.png-752.png', '0801-lm1-others/lm1-3/color/899.png-1034.png', '0801-lm1-others/lm1-3/color/752.png-726.png', '0801-lm1-others/lm1-3/color/892.png-960.png', '0801-lm1-others/lm1-3/color/936.png-763.png', '0801-lm1-others/lm1-3/color/744.png-619.png', '0801-lm1-others/lm1-3/color/995.png-922.png', '0801-lm1-others/lm1-3/color/801.png-640.png', '0801-lm1-others/lm1-3/color/264.png-492.png', '0801-lm1-others/lm1-3/color/29.png-159.png', '0801-lm1-others/lm1-3/color/111.png-5.png', '0801-lm1-others/lm1-3/color/998.png-1019.png', '0801-lm1-others/lm1-3/color/540.png-573.png', '0801-lm1-others/lm1-3/color/783.png-628.png', '0801-lm1-others/lm1-3/color/369.png-424.png', '0801-lm1-others/lm1-3/color/712.png-677.png', '0801-lm1-others/lm1-3/color/544.png-468.png', '0801-lm1-others/lm1-3/color/778.png-713.png', '0801-lm1-others/lm1-3/color/668.png-662.png', '0801-lm1-others/lm1-3/color/704.png-622.png', '0801-lm1-others/lm1-3/color/3.png-5.png', '0801-lm1-others/lm1-3/color/978.png-1008.png', '0801-lm1-others/lm1-3/color/521.png-516.png', '0801-lm1-others/lm1-3/color/438.png-380.png', '0801-lm1-others/lm1-3/color/295.png-362.png', '0801-lm1-others/lm1-3/color/809.png-697.png', '0801-lm1-others/lm1-3/color/119.png-221.png', '0801-lm1-others/lm1-3/color/113.png-119.png', '0801-lm1-others/lm1-3/color/53.png-55.png', '0801-lm1-others/lm1-3/color/580.png-377.png', '0801-lm1-others/lm1-3/color/1010.png-883.png', '0801-lm1-others/lm1-3/color/874.png-898.png', '0801-lm1-others/lm1-3/color/735.png-667.png', '0801-lm1-others/lm1-3/color/1018.png-904.png', '0801-lm1-others/lm1-3/color/958.png-1037.png', '0801-lm1-others/lm1-3/color/759.png-718.png', '0801-lm1-others/lm1-3/color/742.png-642.png', '0801-lm1-others/lm1-3/color/599.png-456.png', '0801-lm1-others/lm1-3/color/641.png-744.png', '0801-lm1-others/lm1-3/color/611.png-690.png', '0801-lm1-others/lm1-3/color/313.png-404.png', '0801-lm1-others/lm1-3/color/197.png-200.png', '0801-lm1-others/lm1-3/color/1010.png-1015.png', '0801-lm1-others/lm1-3/color/213.png-169.png', '0801-lm1-others/lm1-3/color/6.png-71.png', '0801-lm1-others/lm1-3/color/185.png-180.png', '0801-lm1-others/lm1-3/color/797.png-718.png', '0801-lm1-others/lm1-3/color/716.png-760.png', '0801-lm1-others/lm1-3/color/189.png-133.png', '0801-lm1-others/lm1-3/color/260.png-548.png', '0801-lm1-others/lm1-3/color/1035.png-1008.png', '0801-lm1-others/lm1-3/color/828.png-941.png', '0801-lm1-others/lm1-3/color/487.png-270.png', '0801-lm1-others/lm1-3/color/787.png-691.png', '0801-lm1-others/lm1-3/color/44.png-163.png', '0801-lm1-others/lm1-3/color/88.png-254.png', '0801-lm1-others/lm1-3/color/25.png-225.png', '0801-lm1-others/lm1-3/color/267.png-487.png', '0801-lm1-others/lm1-3/color/170.png-243.png', '0801-lm1-others/lm1-3/color/438.png-333.png', '0801-lm1-others/lm1-3/color/86.png-983.png', '0801-lm1-others/lm1-3/color/134.png-163.png', '0801-lm1-others/lm1-3/color/686.png-935.png', '0801-lm1-others/lm1-3/color/360.png-475.png', '0801-lm1-others/lm1-3/color/63.png-103.png', '0801-lm1-others/lm1-3/color/160.png-37.png', '0801-lm1-others/lm1-3/color/877.png-874.png', '0801-lm1-others/lm1-3/color/532.png-486.png', '0801-lm1-others/lm1-3/color/300.png-400.png', '0801-lm1-others/lm1-3/color/964.png-1003.png', '0801-lm1-others/lm1-3/color/159.png-78.png', '0801-lm1-others/lm1-3/color/623.png-649.png', '0801-lm1-others/lm1-3/color/589.png-429.png', '0801-lm1-others/lm1-3/color/211.png-248.png', '0801-lm1-others/lm1-3/color/559.png-545.png', '0801-lm1-others/lm1-3/color/356.png-411.png', '0801-lm1-others/lm1-3/color/570.png-614.png', '0801-lm1-others/lm1-3/color/628.png-626.png', '0801-lm1-others/lm1-3/color/553.png-551.png', '0801-lm1-others/lm1-3/color/723.png-795.png'], '3': ['0801-lm1-others/lm1-3/color/943.png-1002.png', '0801-lm1-others/lm1-3/color/899.png-866.png', '0801-lm1-others/lm1-3/color/82.png-153.png', '0801-lm1-others/lm1-3/color/912.png-907.png', '0801-lm1-others/lm1-3/color/295.png-561.png', '0801-lm1-others/lm1-3/color/260.png-492.png', '0801-lm1-others/lm1-3/color/57.png-61.png', '0801-lm1-others/lm1-3/color/1002.png-1022.png', '0801-lm1-others/lm1-3/color/717.png-651.png', '0801-lm1-others/lm1-3/color/870.png-968.png', '0801-lm1-others/lm1-3/color/450.png-443.png', '0801-lm1-others/lm1-3/color/524.png-314.png', '0801-lm1-others/lm1-3/color/836.png-899.png', '0801-lm1-others/lm1-3/color/367.png-383.png', '0801-lm1-others/lm1-3/color/734.png-754.png', '0801-lm1-others/lm1-3/color/345.png-412.png', '0801-lm1-others/lm1-3/color/805.png-654.png', '0801-lm1-others/lm1-3/color/1026.png-1032.png', '0801-lm1-others/lm1-3/color/217.png-151.png', '0801-lm1-others/lm1-3/color/34.png-253.png', '0801-lm1-others/lm1-3/color/653.png-752.png', '0801-lm1-others/lm1-3/color/813.png-820.png', '0801-lm1-others/lm1-3/color/294.png-400.png', '0801-lm1-others/lm1-3/color/243.png-65.png', '0801-lm1-others/lm1-3/color/791.png-463.png', '0801-lm1-others/lm1-3/color/701.png-698.png', '0801-lm1-others/lm1-3/color/793.png-807.png', '0801-lm1-others/lm1-3/color/83.png-166.png', '0801-lm1-others/lm1-3/color/911.png-1019.png', '0801-lm1-others/lm1-3/color/450.png-457.png', '0801-lm1-others/lm1-3/color/784.png-629.png', '0801-lm1-others/lm1-3/color/35.png-228.png', '0801-lm1-others/lm1-3/color/651.png-681.png', '0801-lm1-others/lm1-3/color/483.png-417.png', '0801-lm1-others/lm1-3/color/346.png-562.png', '0801-lm1-others/lm1-3/color/43.png-73.png', '0801-lm1-others/lm1-3/color/680.png-623.png', '0801-lm1-others/lm1-3/color/649.png-739.png', '0801-lm1-others/lm1-3/color/772.png-935.png', '0801-lm1-others/lm1-3/color/427.png-425.png', '0801-lm1-others/lm1-3/color/594.png-602.png', '0801-lm1-others/lm1-3/color/332.png-568.png', '0801-lm1-others/lm1-3/color/685.png-827.png', '0801-lm1-others/lm1-3/color/546.png-32.png', '0801-lm1-others/lm1-3/color/539.png-263.png', '0801-lm1-others/lm1-3/color/108.png-255.png', '0801-lm1-others/lm1-3/color/836.png-867.png', '0801-lm1-others/lm1-3/color/926.png-862.png', '0801-lm1-others/lm1-3/color/154.png-401.png', '0801-lm1-others/lm1-3/color/422.png-423.png', '0801-lm1-others/lm1-3/color/941.png-945.png', '0801-lm1-others/lm1-3/color/866.png-84.png', '0801-lm1-others/lm1-3/color/888.png-1011.png', '0801-lm1-others/lm1-3/color/536.png-427.png', '0801-lm1-others/lm1-3/color/607.png-699.png', '0801-lm1-others/lm1-3/color/8.png-65.png', '0801-lm1-others/lm1-3/color/862.png-929.png', '0801-lm1-others/lm1-3/color/676.png-614.png', '0801-lm1-others/lm1-3/color/569.png-611.png', '0801-lm1-others/lm1-3/color/655.png-712.png', '0801-lm1-others/lm1-3/color/821.png-901.png', '0801-lm1-others/lm1-3/color/280.png-471.png', '0801-lm1-others/lm1-3/color/998.png-863.png', '0801-lm1-others/lm1-3/color/901.png-1020.png', '0801-lm1-others/lm1-3/color/20.png-985.png', '0801-lm1-others/lm1-3/color/518.png-779.png', '0801-lm1-others/lm1-3/color/791.png-666.png', '0801-lm1-others/lm1-3/color/754.png-655.png', '0801-lm1-others/lm1-3/color/875.png-177.png', '0801-lm1-others/lm1-3/color/811.png-941.png', '0801-lm1-others/lm1-3/color/726.png-703.png', '0801-lm1-others/lm1-3/color/830.png-815.png', '0801-lm1-others/lm1-3/color/649.png-634.png', '0801-lm1-others/lm1-3/color/215.png-165.png', '0801-lm1-others/lm1-3/color/439.png-431.png', '0801-lm1-others/lm1-3/color/907.png-885.png', '0801-lm1-others/lm1-3/color/841.png-1009.png', '0801-lm1-others/lm1-3/color/119.png-23.png', '0801-lm1-others/lm1-3/color/689.png-808.png', '0801-lm1-others/lm1-3/color/864.png-956.png'], '4': ['0801-lm1-others/lm1-3/color/268.png-492.png', '0801-lm1-others/lm1-3/color/382.png-375.png', '0801-lm1-others/lm1-3/color/640.png-740.png', '0801-lm1-others/lm1-3/color/888.png-881.png', '0801-lm1-others/lm1-3/color/946.png-834.png', '0801-lm1-others/lm1-3/color/1038.png-103.png', '0801-lm1-others/lm1-3/color/478.png-468.png', '0801-lm1-others/lm1-3/color/234.png-150.png', '0801-lm1-others/lm1-3/color/456.png-461.png', '0801-lm1-others/lm1-3/color/525.png-579.png', '0801-lm1-others/lm1-3/color/550.png-432.png', '0801-lm1-others/lm1-3/color/138.png-252.png', '0801-lm1-others/lm1-3/color/427.png-266.png', '0801-lm1-others/lm1-3/color/332.png-524.png', '0801-lm1-others/lm1-3/color/697.png-642.png', '0801-lm1-others/lm1-3/color/944.png-932.png', '0801-lm1-others/lm1-3/color/28.png-288.png', '0801-lm1-others/lm1-3/color/819.png-903.png', '0801-lm1-others/lm1-3/color/174.png-1038.png', '0801-lm1-others/lm1-3/color/28.png-161.png', '0801-lm1-others/lm1-3/color/965.png-909.png', '0801-lm1-others/lm1-3/color/738.png-775.png', '0801-lm1-others/lm1-3/color/197.png-240.png', '0801-lm1-others/lm1-3/color/429.png-420.png', '0801-lm1-others/lm1-3/color/673.png-598.png', '0801-lm1-others/lm1-3/color/814.png-829.png', '0801-lm1-others/lm1-3/color/581.png-551.png', '0801-lm1-others/lm1-3/color/627.png-695.png', '0801-lm1-others/lm1-3/color/322.png-276.png', '0801-lm1-others/lm1-3/color/1001.png-1012.png', '0801-lm1-others/lm1-3/color/423.png-275.png', '0801-lm1-others/lm1-3/color/710.png-777.png', '0801-lm1-others/lm1-3/color/743.png-713.png', '0801-lm1-others/lm1-3/color/170.png-116.png', '0801-lm1-others/lm1-3/color/942.png-1020.png', '0801-lm1-others/lm1-3/color/167.png-81.png', '0801-lm1-others/lm1-3/color/729.png-651.png', '0801-lm1-others/lm1-3/color/777.png-747.png', '0801-lm1-others/lm1-3/color/822.png-915.png', '0801-lm1-others/lm1-3/color/395.png-304.png', '0801-lm1-others/lm1-3/color/795.png-782.png', '0801-lm1-others/lm1-3/color/127.png-253.png', '0801-lm1-others/lm1-3/color/76.png-23.png', '0801-lm1-others/lm1-3/color/77.png-229.png', '0801-lm1-others/lm1-3/color/416.png-345.png', '0801-lm1-others/lm1-3/color/33.png-30.png', '0801-lm1-others/lm1-3/color/596.png-389.png', '0801-lm1-others/lm1-3/color/573.png-539.png', '0801-lm1-others/lm1-3/color/242.png-156.png', '0801-lm1-others/lm1-3/color/599.png-597.png', '0801-lm1-others/lm1-3/color/6.png-162.png', '0801-lm1-others/lm1-3/color/431.png-445.png', '0801-lm1-others/lm1-3/color/838.png-858.png', '0801-lm1-others/lm1-3/color/113.png-6.png', '0801-lm1-others/lm1-3/color/898.png-835.png', '0801-lm1-others/lm1-3/color/754.png-693.png', '0801-lm1-others/lm1-3/color/868.png-875.png', '0801-lm1-others/lm1-3/color/150.png-230.png', '0801-lm1-others/lm1-3/color/439.png-436.png', '0801-lm1-others/lm1-3/color/630.png-736.png', '0801-lm1-others/lm1-3/color/276.png-321.png', '0801-lm1-others/lm1-3/color/1046.png-820.png', '0801-lm1-others/lm1-3/color/401.png-152.png', '0801-lm1-others/lm1-3/color/704.png-731.png', '0801-lm1-others/lm1-3/color/60.png-983.png', '0801-lm1-others/lm1-3/color/256.png-404.png', '0801-lm1-others/lm1-3/color/577.png-614.png', '0801-lm1-others/lm1-3/color/710.png-739.png', '0801-lm1-others/lm1-3/color/231.png-103.png', '0801-lm1-others/lm1-3/color/153.png-244.png', '0801-lm1-others/lm1-3/color/169.png-289.png', '0801-lm1-others/lm1-3/color/957.png-177.png', '0801-lm1-others/lm1-3/color/451.png-449.png', '0801-lm1-others/lm1-3/color/675.png-676.png', '0801-lm1-others/lm1-3/color/519.png-787.png', '0801-lm1-others/lm1-3/color/346.png-354.png', '0801-lm1-others/lm1-3/color/406.png-539.png', '0801-lm1-others/lm1-3/color/995.png-912.png', '0801-lm1-others/lm1-3/color/29.png-78.png', '0801-lm1-others/lm1-3/color/959.png-60.png'], '5': ['0801-lm1-others/lm1-3/color/947.png-883.png', '0801-lm1-others/lm1-3/color/60.png-233.png', '0801-lm1-others/lm1-3/color/912.png-826.png', '0801-lm1-others/lm1-3/color/189.png-78.png', '0801-lm1-others/lm1-3/color/75.png-37.png', '0801-lm1-others/lm1-3/color/151.png-148.png', '0801-lm1-others/lm1-3/color/35.png-163.png', '0801-lm1-others/lm1-3/color/706.png-660.png', '0801-lm1-others/lm1-3/color/950.png-898.png', '0801-lm1-others/lm1-3/color/358.png-583.png', '0801-lm1-others/lm1-3/color/632.png-685.png', '0801-lm1-others/lm1-3/color/560.png-474.png', '0801-lm1-others/lm1-3/color/878.png-983.png', '0801-lm1-others/lm1-3/color/297.png-289.png', '0801-lm1-others/lm1-3/color/63.png-59.png', '0801-lm1-others/lm1-3/color/725.png-933.png', '0801-lm1-others/lm1-3/color/731.png-624.png', '0801-lm1-others/lm1-3/color/388.png-342.png', '0801-lm1-others/lm1-3/color/1028.png-842.png', '0801-lm1-others/lm1-3/color/466.png-431.png', '0801-lm1-others/lm1-3/color/107.png-103.png', '0801-lm1-others/lm1-3/color/693.png-600.png', '0801-lm1-others/lm1-3/color/611.png-577.png', '0801-lm1-others/lm1-3/color/610.png-568.png', '0801-lm1-others/lm1-3/color/398.png-307.png', '0801-lm1-others/lm1-3/color/883.png-1015.png', '0801-lm1-others/lm1-3/color/766.png-928.png', '0801-lm1-others/lm1-3/color/105.png-32.png', '0801-lm1-others/lm1-3/color/531.png-527.png', '0801-lm1-others/lm1-3/color/77.png-216.png', '0801-lm1-others/lm1-3/color/789.png-571.png', '0801-lm1-others/lm1-3/color/898.png-966.png', '0801-lm1-others/lm1-3/color/55.png-1026.png', '0801-lm1-others/lm1-3/color/272.png-392.png', '0801-lm1-others/lm1-3/color/786.png-712.png', '0801-lm1-others/lm1-3/color/317.png-582.png', '0801-lm1-others/lm1-3/color/976.png-869.png', '0801-lm1-others/lm1-3/color/751.png-769.png', '0801-lm1-others/lm1-3/color/9.png-220.png', '0801-lm1-others/lm1-3/color/611.png-336.png', '0801-lm1-others/lm1-3/color/533.png-542.png', '0801-lm1-others/lm1-3/color/632.png-627.png', '0801-lm1-others/lm1-3/color/908.png-858.png', '0801-lm1-others/lm1-3/color/34.png-93.png', '0801-lm1-others/lm1-3/color/97.png-66.png', '0801-lm1-others/lm1-3/color/815.png-1025.png', '0801-lm1-others/lm1-3/color/980.png-942.png', '0801-lm1-others/lm1-3/color/555.png-528.png', '0801-lm1-others/lm1-3/color/233.png-93.png', '0801-lm1-others/lm1-3/color/939.png-819.png', '0801-lm1-others/lm1-3/color/94.png-1031.png', '0801-lm1-others/lm1-3/color/533.png-561.png', '0801-lm1-others/lm1-3/color/815.png-1002.png', '0801-lm1-others/lm1-3/color/444.png-315.png', '0801-lm1-others/lm1-3/color/699.png-452.png', '0801-lm1-others/lm1-3/color/575.png-270.png', '0801-lm1-others/lm1-3/color/892.png-182.png', '0801-lm1-others/lm1-3/color/980.png-955.png', '0801-lm1-others/lm1-3/color/515.png-614.png', '0801-lm1-others/lm1-3/color/1013.png-729.png', '0801-lm1-others/lm1-3/color/692.png-636.png', '0801-lm1-others/lm1-3/color/333.png-407.png', '0801-lm1-others/lm1-3/color/878.png-84.png', '0801-lm1-others/lm1-3/color/44.png-155.png', '0801-lm1-others/lm1-3/color/661.png-641.png', '0801-lm1-others/lm1-3/color/958.png-208.png', '0801-lm1-others/lm1-3/color/656.png-746.png', '0801-lm1-others/lm1-3/color/17.png-193.png', '0801-lm1-others/lm1-3/color/456.png-526.png', '0801-lm1-others/lm1-3/color/203.png-478.png', '0801-lm1-others/lm1-3/color/813.png-943.png', '0801-lm1-others/lm1-3/color/140.png-156.png', '0801-lm1-others/lm1-3/color/241.png-223.png', '0801-lm1-others/lm1-3/color/955.png-814.png', '0801-lm1-others/lm1-3/color/484.png-573.png', '0801-lm1-others/lm1-3/color/744.png-807.png', '0801-lm1-others/lm1-3/color/553.png-533.png', '0801-lm1-others/lm1-3/color/564.png-256.png', '0801-lm1-others/lm1-3/color/402.png-544.png', '0801-lm1-others/lm1-3/color/585.png-310.png']}\n",
      "\n",
      "sample_data = 0801-lm1-others/lm1-3/color/458.png-700.png\n",
      "label = 0801-lm1-others\n",
      "name = lm1\n",
      "dir_name = 0801-lm1-others/lm1-3/color\n",
      "FULL_ROOT_DIR = data/LM_dataset/0801-lm1-others/lm1-3/color\n",
      "\n",
      "rotation_key = 0\n",
      "rotation_list = ['0801-lm1-others/lm1-3/color/458.png-700.png', '0801-lm1-others/lm1-3/color/611.png-608.png', '0801-lm1-others/lm1-3/color/630.png-631.png', '0801-lm1-others/lm1-3/color/389.png-332.png', '0801-lm1-others/lm1-3/color/895.png-894.png', '0801-lm1-others/lm1-3/color/970.png-867.png', '0801-lm1-others/lm1-3/color/140.png-139.png', '0801-lm1-others/lm1-3/color/469.png-544.png', '0801-lm1-others/lm1-3/color/247.png-232.png', '0801-lm1-others/lm1-3/color/935.png-770.png', '0801-lm1-others/lm1-3/color/135.png-107.png', '0801-lm1-others/lm1-3/color/201.png-200.png', '0801-lm1-others/lm1-3/color/868.png-970.png', '0801-lm1-others/lm1-3/color/713.png-627.png', '0801-lm1-others/lm1-3/color/955.png-954.png', '0801-lm1-others/lm1-3/color/119.png-114.png', '0801-lm1-others/lm1-3/color/768.png-766.png', '0801-lm1-others/lm1-3/color/196.png-195.png', '0801-lm1-others/lm1-3/color/623.png-624.png', '0801-lm1-others/lm1-3/color/877.png-1037.png', '0801-lm1-others/lm1-3/color/909.png-907.png', '0801-lm1-others/lm1-3/color/333.png-445.png', '0801-lm1-others/lm1-3/color/312.png-313.png', '0801-lm1-others/lm1-3/color/322.png-321.png', '0801-lm1-others/lm1-3/color/993.png-1028.png', '0801-lm1-others/lm1-3/color/663.png-664.png', '0801-lm1-others/lm1-3/color/974.png-973.png', '0801-lm1-others/lm1-3/color/1044.png-899.png', '0801-lm1-others/lm1-3/color/412.png-413.png', '0801-lm1-others/lm1-3/color/541.png-540.png', '0801-lm1-others/lm1-3/color/938.png-765.png', '0801-lm1-others/lm1-3/color/967.png-839.png', '0801-lm1-others/lm1-3/color/969.png-1002.png', '0801-lm1-others/lm1-3/color/10.png-9.png', '0801-lm1-others/lm1-3/color/303.png-301.png', '0801-lm1-others/lm1-3/color/119.png-215.png', '0801-lm1-others/lm1-3/color/964.png-962.png', '0801-lm1-others/lm1-3/color/338.png-339.png', '0801-lm1-others/lm1-3/color/1006.png-898.png', '0801-lm1-others/lm1-3/color/285.png-401.png', '0801-lm1-others/lm1-3/color/780.png-627.png', '0801-lm1-others/lm1-3/color/725.png-805.png', '0801-lm1-others/lm1-3/color/444.png-443.png', '0801-lm1-others/lm1-3/color/48.png-49.png', '0801-lm1-others/lm1-3/color/1030.png-1045.png', '0801-lm1-others/lm1-3/color/370.png-329.png', '0801-lm1-others/lm1-3/color/795.png-653.png', '0801-lm1-others/lm1-3/color/173.png-45.png', '0801-lm1-others/lm1-3/color/856.png-855.png', '0801-lm1-others/lm1-3/color/14.png-13.png', '0801-lm1-others/lm1-3/color/502.png-501.png', '0801-lm1-others/lm1-3/color/209.png-176.png', '0801-lm1-others/lm1-3/color/456.png-455.png', '0801-lm1-others/lm1-3/color/81.png-240.png', '0801-lm1-others/lm1-3/color/595.png-596.png', '0801-lm1-others/lm1-3/color/75.png-74.png', '0801-lm1-others/lm1-3/color/45.png-173.png', '0801-lm1-others/lm1-3/color/573.png-430.png', '0801-lm1-others/lm1-3/color/802.png-801.png', '0801-lm1-others/lm1-3/color/278.png-277.png', '0801-lm1-others/lm1-3/color/269.png-268.png', '0801-lm1-others/lm1-3/color/795.png-682.png', '0801-lm1-others/lm1-3/color/715.png-716.png', '0801-lm1-others/lm1-3/color/410.png-352.png', '0801-lm1-others/lm1-3/color/811.png-812.png', '0801-lm1-others/lm1-3/color/1013.png-913.png', '0801-lm1-others/lm1-3/color/113.png-112.png', '0801-lm1-others/lm1-3/color/134.png-133.png', '0801-lm1-others/lm1-3/color/130.png-43.png', '0801-lm1-others/lm1-3/color/497.png-431.png', '0801-lm1-others/lm1-3/color/766.png-767.png', '0801-lm1-others/lm1-3/color/801.png-802.png', '0801-lm1-others/lm1-3/color/716.png-715.png', '0801-lm1-others/lm1-3/color/669.png-605.png', '0801-lm1-others/lm1-3/color/378.png-379.png', '0801-lm1-others/lm1-3/color/202.png-201.png', '0801-lm1-others/lm1-3/color/821.png-1019.png', '0801-lm1-others/lm1-3/color/964.png-899.png', '0801-lm1-others/lm1-3/color/592.png-608.png', '0801-lm1-others/lm1-3/color/766.png-767.png']\n",
      "\n",
      "pair_idx = 0\n",
      "pair_name = 0801-lm1-others/lm1-3/color/458.png-700.png\n",
      "\n",
      "idx0_name = 458.png\n",
      "idx1_name = 700.png\n",
      "image0_name = data/LM_dataset/0801-lm1-others/lm1-3/color/458.png\n",
      "image1_name = data/LM_dataset/0801-lm1-others/lm1-3/color_full/700.png\n",
      "intrinsic0_path = data/LM_dataset/0801-lm1-others/lm1-3/intrin_ba/458.txt\n",
      "intrinsic1_path = data/LM_dataset/0801-lm1-others/lm1-3/intrin/700.txt\n"
     ]
    }
   ],
   "source": [
    "for label_idx, test_dict in enumerate(dir_list[:1]):\n",
    "    print(\"label_idx =\", label_idx)\n",
    "    print(\"test_dict =\", test_dict)\n",
    "    print()\n",
    "    logger.info(f\"LINEMOD: {label_idx}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    sample_data = dir_list[label_idx][\"0\"][0]\n",
    "    label = sample_data.split(\"/\")[0]\n",
    "    name = label.split(\"-\")[1]\n",
    "    dir_name = os.path.dirname(sample_data)\n",
    "    FULL_ROOT_DIR = os.path.join(ROOT_DIR, dir_name)\n",
    "    print(\"sample_data =\", sample_data)\n",
    "    print(\"label =\", label)\n",
    "    print(\"name =\", name)\n",
    "    print(\"dir_name =\", dir_name)\n",
    "    print(\"FULL_ROOT_DIR =\", FULL_ROOT_DIR)\n",
    "    print()\n",
    "    recall_image, all_image = 0, 0\n",
    "    for rotation_key, rotation_list in zip(test_dict.keys(), test_dict.values()):\n",
    "        if rotation_key == '1':\n",
    "            break\n",
    "        print(\"rotation_key =\", rotation_key)\n",
    "        print(\"rotation_list =\", rotation_list)\n",
    "        print()\n",
    "        for pair_idx, pair_name in enumerate(rotation_list[:1]):\n",
    "            print(\"pair_idx =\", pair_idx)\n",
    "            print(\"pair_name =\", pair_name)\n",
    "            print()\n",
    "            all_image = all_image + 1\n",
    "            base_name = os.path.basename(pair_name)\n",
    "            idx0_name = base_name.split(\"-\")[0]\n",
    "            idx1_name = base_name.split(\"-\")[1]\n",
    "            print(\"idx0_name =\", idx0_name)\n",
    "            print(\"idx1_name =\", idx1_name)\n",
    "            image0_name = os.path.join(FULL_ROOT_DIR, idx0_name)\n",
    "            image1_name = os.path.join(FULL_ROOT_DIR.replace(\"color\", \"color_full\"), idx1_name)\n",
    "            print(\"image0_name =\", image0_name)\n",
    "            print(\"image1_name =\", image1_name)\n",
    "            intrinsic_path = image0_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "            print(\"intrinsic0_path =\", intrinsic_path)\n",
    "            intrinsic_path = image1_name.replace(\"color_full\", \"intrin\").replace(\"png\", \"txt\")\n",
    "            print(\"intrinsic1_path =\", intrinsic_path)\n",
    "            image0 = cv2.imread(image0_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-19 14:15:49.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mLINEMOD: 0\u001b[0m\n",
      " 18%|█▊        | 14/80 [00:37<02:56,  2.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m crop_tensor \u001b[38;5;241m=\u001b[39m set_torch_image(image_crop, center_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 53\u001b[0m     fea \u001b[38;5;241m=\u001b[39m \u001b[43mget_cls_token_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdinov2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m score \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(ref_fea, fea, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (score\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m similarity_score)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[0;32m~/POPE/segment_anything/segment_anything/dinov2_utils.py:108\u001b[0m, in \u001b[0;36mget_cls_token_torch\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cls_token_torch\u001b[39m(model, input_tensor):\n\u001b[1;32m    107\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 108\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     cls_token \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_norm_clstoken\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# norm_cls_token = torch.nn.functional.normalize(cls_token)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/POPE/dinov2/dinov2/models/vision_transformer.py:291\u001b[0m, in \u001b[0;36mDinoVisionTransformer.forward\u001b[0;34m(self, is_training, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 291\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/POPE/dinov2/dinov2/models/vision_transformer.py:228\u001b[0m, in \u001b[0;36mDinoVisionTransformer.forward_features\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    225\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_tokens_with_masks(x, masks)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 228\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_norm_clstoken\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_norm[:, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_norm_patchtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: x_norm[:, \u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_prenorm\u001b[39m\u001b[38;5;124m\"\u001b[39m: x,\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m: masks,\n\u001b[1;32m    236\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/POPE/dinov2/dinov2/layers/block.py:247\u001b[0m, in \u001b[0;36mNestedTensorBlock.forward\u001b[0;34m(self, x_or_x_list)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_or_x_list):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_or_x_list, Tensor):\n\u001b[0;32m--> 247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_or_x_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_or_x_list, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m XFORMERS_AVAILABLE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install xFormers for nested tensors usage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/POPE/dinov2/dinov2/layers/block.py:105\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(ffn_residual_func(x))  \u001b[38;5;66;03m# FIXME: drop_path2\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mattn_residual_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m ffn_residual_func(x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/POPE/dinov2/dinov2/layers/block.py:84\u001b[0m, in \u001b[0;36mBlock.forward.<locals>.attn_residual_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattn_residual_func\u001b[39m(x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/POPE/dinov2/dinov2/layers/attention.py:80\u001b[0m, in \u001b[0;36mMemEffAttention.forward\u001b[0;34m(self, x, attn_bias)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     self_att_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_att_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape([B, N, C])\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:223\u001b[0m, in \u001b[0;36mmemory_efficient_attention\u001b[0;34m(query, key, value, attn_bias, p, scale, op)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemory_efficient_attention\u001b[39m(\n\u001b[1;32m    117\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    118\u001b[0m     key: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     op: Optional[AttentionOp] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implements the memory-efficient attention mechanism following\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    `\"Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:321\u001b[0m, in \u001b[0;36m_memory_efficient_attention\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_memory_efficient_attention\u001b[39m(\n\u001b[1;32m    317\u001b[0m     inp: Inputs, op: Optional[AttentionOp] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# fast-path that doesn't require computing the logsumexp for backward computation\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [inp\u001b[38;5;241m.\u001b[39mquery, inp\u001b[38;5;241m.\u001b[39mkey, inp\u001b[38;5;241m.\u001b[39mvalue]):\n\u001b[0;32m--> 321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     output_shape \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mnormalize_bmhk()\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _fMHA\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    327\u001b[0m         op, inp\u001b[38;5;241m.\u001b[39mquery, inp\u001b[38;5;241m.\u001b[39mkey, inp\u001b[38;5;241m.\u001b[39mvalue, inp\u001b[38;5;241m.\u001b[39mattn_bias, inp\u001b[38;5;241m.\u001b[39mp, inp\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    328\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(output_shape)\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:341\u001b[0m, in \u001b[0;36m_memory_efficient_attention_forward\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     _ensure_op_supports_or_raise(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_efficient_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m, op, inp)\n\u001b[0;32m--> 341\u001b[0m out, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mreshape(output_shape)\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:202\u001b[0m, in \u001b[0;36mFwOp.apply\u001b[0;34m(cls, inp, needs_gradient)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported attn_bias type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_bmhk\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery has shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minp\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m ctx: Optional[Context] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pope/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:259\u001b[0m, in \u001b[0;36mFwOp.apply_bmhk\u001b[0;34m(cls, inp, needs_gradient)\u001b[0m\n\u001b[1;32m    252\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m Context(\n\u001b[1;32m    253\u001b[0m             out\u001b[38;5;241m=\u001b[39mout,\n\u001b[1;32m    254\u001b[0m             lse\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([o[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlse \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outs], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    255\u001b[0m             op_bw\u001b[38;5;241m=\u001b[39mouts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mop_bw,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         )\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, ctx\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_bmhk\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mcls\u001b[39m, inp: Inputs, needs_gradient: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[Context]]:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(inp\u001b[38;5;241m.\u001b[39mattn_bias) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m FwOp\u001b[38;5;241m.\u001b[39mSUPPORTED_ATTN_BIAS_TYPES:\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported attn_bias type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for label_idx, test_dict in enumerate(dir_list):\n",
    "    logger.info(f\"LINEMOD: {label_idx}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    sample_data = dir_list[label_idx][\"0\"][0]\n",
    "    label = sample_data.split(\"/\")[0]\n",
    "    name = label.split(\"-\")[1]\n",
    "    dir_name = os.path.dirname(sample_data)\n",
    "    FULL_ROOT_DIR = os.path.join(ROOT_DIR, dir_name)\n",
    "    recall_image, all_image = 0, 0\n",
    "    for rotation_key, rotation_list in zip(test_dict.keys(), test_dict.values()):\n",
    "        for pair_idx, pair_name in enumerate(tqdm(rotation_list)):\n",
    "            all_image = all_image + 1\n",
    "            base_name = os.path.basename(pair_name)\n",
    "            idx0_name = base_name.split(\"-\")[0]\n",
    "            idx1_name = base_name.split(\"-\")[1]\n",
    "            image0_name = os.path.join(FULL_ROOT_DIR, idx0_name)\n",
    "            image1_name = os.path.join(FULL_ROOT_DIR.replace(\"color\", \"color_full\"), idx1_name)\n",
    "            intrinsic_path = image0_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "            K0 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "            intrinsic_path = image1_name.replace(\"color_full\", \"intrin\").replace(\"png\", \"txt\")\n",
    "            K1 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "            image0 = cv2.imread(image0_name)\n",
    "            ref_torch_image = set_torch_image(image0, center_crop=True)\n",
    "            ref_fea = get_cls_token_torch(dinov2_model, ref_torch_image)\n",
    "            image1 = cv2.imread(image1_name)\n",
    "            image_h, image_w, _ = image1.shape\n",
    "            t1 = time.time()\n",
    "            masks = MASK_GEN.generate(image1)\n",
    "            t2 = time.time()\n",
    "            similarity_score, top_images  = np.array([0, 0, 0], np.float32), [[], [], []]\n",
    "            t3 = time.time()\n",
    "            compact_percent = 0.3\n",
    "            for xxx, mask in enumerate(masks):\n",
    "                object_mask = np.expand_dims(mask[\"segmentation\"], -1)\n",
    "                x0, y0, w, h = mask[\"bbox\"]\n",
    "                x1, y1 = x0 + w, y0 + h\n",
    "                x0 -= int(w * compact_percent)\n",
    "                y0 -= int(h * compact_percent)\n",
    "                x1 += int(w * compact_percent)\n",
    "                y1 += int(h * compact_percent)\n",
    "                box = np.array([x0, y0, x1, y1])\n",
    "                resize_shape = np.array([y1 - y0, x1 - x0])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box, K1, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image1, box, resize_shape)\n",
    "                # object_mask, _ = get_image_crop_resize(object_mask, box, resize_shape)\n",
    "                box_new = np.array([0, 0, x1 - x0, y1 - y0])\n",
    "                resize_shape = np.array([256, 256])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box_new, K_crop, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image_crop, box_new, resize_shape)\n",
    "                crop_tensor = set_torch_image(image_crop, center_crop=True)\n",
    "                with torch.no_grad():\n",
    "                    fea = get_cls_token_torch(dinov2_model, crop_tensor)\n",
    "                score = F.cosine_similarity(ref_fea, fea, dim=1, eps=1e-8)\n",
    "                if (score.item() > similarity_score).any():\n",
    "                    mask[\"crop_image\"] = image_crop\n",
    "                    mask[\"K\"] = K_crop\n",
    "                    mask[\"bbox\"] = box\n",
    "                    min_idx = np.argmin(similarity_score)\n",
    "                    similarity_score[min_idx] = score.item()\n",
    "                    top_images[min_idx] = mask.copy()\n",
    "\n",
    "            img0 = cv2.cvtColor(image0, cv2.COLOR_BGR2GRAY)\n",
    "            img0 = torch.from_numpy(img0).float()[None] / 255.\n",
    "            img0 = img0.unsqueeze(0).cuda()\n",
    "\n",
    "            matching_score = [[0] for _ in range(len(top_images))]\n",
    "            for top_idx in range(len(top_images)):\n",
    "                img1 = cv2.cvtColor(top_images[top_idx][\"crop_image\"], cv2.COLOR_BGR2GRAY)\n",
    "                img1 = torch.from_numpy(img1).float()[None] / 255.\n",
    "                img1 = img1.unsqueeze(0).cuda()\n",
    "                batch = {'image0':img0, 'image1':img1}\n",
    "                with torch.no_grad():\n",
    "                    matcher(batch)\n",
    "                    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "                    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "                    confidences = batch[\"mconf\"].cpu().numpy()\n",
    "                conf_mask = np.where(confidences > 0.9)\n",
    "                matching_score[top_idx] = conf_mask[0].shape[0]\n",
    "                top_images[top_idx][\"mkpts0\"] = mkpts0\n",
    "                top_images[top_idx][\"mkpts1\"] = mkpts1\n",
    "                top_images[top_idx][\"mconf\"] = confidences\n",
    "            #---------------------------------------------------\n",
    "            # crop_image = cv2.resize(top_images[np.argmax(matching_score)][\"crop_image\"], (256, 256))\n",
    "            # que_image = cv2.resize(image0, (256, 256))\n",
    "            # image = np.hstack((que_image, crop_image))\n",
    "            # for top_idx in range(len(top_images)):\n",
    "            #     crop_image = top_images[top_idx][\"crop_image\"]\n",
    "            #     score = matching_score[top_idx]\n",
    "            #     crop_image = cv2.resize(crop_image, (256, 256))\n",
    "            #     cv2.putText(crop_image, f'{score}', (100, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1)\n",
    "            #     image = np.hstack((image, crop_image))\n",
    "            # cv2.imwrite(f\"segment_anything/crop_images/{idx}.jpg\", image)\n",
    "            #---------------------------------------------------\n",
    "            t4 = time.time()\n",
    "            # print(f\"t4-t3: object detection:{1000*(t4-t3)} ms\")\n",
    "            pose0_name = image0_name.replace(\"color\", \"poses_ba\").replace(\"png\", \"txt\")\n",
    "            pose1_name = image1_name.replace(\"color_full\", \"poses_ba\").replace(\"png\", \"txt\")\n",
    "            pose0 = np.loadtxt(pose0_name)\n",
    "            pose1 = np.loadtxt(pose1_name)\n",
    "            pose0 = np.vstack((pose0, np.array([[0, 0, 0, 1]])))\n",
    "            pose1 = np.vstack((pose1, np.array([[0, 0, 0, 1]])))\n",
    "            relative_pose = np.matmul(pose1, inv(pose0))\n",
    "            t = relative_pose[:3, -1].reshape(1, 3)\n",
    "\n",
    "            max_match_idx = np.argmax(matching_score)\n",
    "            pre_bbox = top_images[max_match_idx][\"bbox\"]\n",
    "            mkpts0 = top_images[max_match_idx][\"mkpts0\"]\n",
    "            mkpts1 = top_images[max_match_idx][\"mkpts1\"]\n",
    "            pre_K = top_images[max_match_idx][\"K\"]\n",
    "\n",
    "            _3d_bbox = np.loadtxt(f\"{os.path.join(ROOT_DIR, label)}/box3d_corners.txt\")\n",
    "            bbox_pts_3d, _ = project_points(_3d_bbox, pose1[:3, :4], K1)\n",
    "            bbox_pts_3d = bbox_pts_3d.astype(np.int32)\n",
    "            x0, y0, w, h = cv2.boundingRect(bbox_pts_3d)\n",
    "            x1, y1 = x0 + w, y0 + h\n",
    "            gt_bbox = np.array([x0, y0, x1, y1])\n",
    "            is_recalled = recall_object(pre_bbox, gt_bbox)\n",
    "            recall_image = recall_image + int(is_recalled > 0.5)\n",
    "            ret = estimate_pose(mkpts0, mkpts1, K0, pre_K, 0.5, 0.99)\n",
    "            if ret is not None:\n",
    "                Rot, t, inliers = ret\n",
    "                t_err, R_err = relative_pose_error(relative_pose, Rot, t, ignore_gt_t_thr=0.0)\n",
    "                metrics['R_errs'].append(R_err)\n",
    "                metrics['t_errs'].append(t_err)\n",
    "            else:\n",
    "                metrics['R_errs'].append(90)\n",
    "                metrics['t_errs'].append(90)\n",
    "            metrics[\"identifiers\"].append(pair_name)\n",
    "\n",
    "    import pprint\n",
    "    from src.utils.metrics import (\n",
    "        aggregate_metrics\n",
    "    )\n",
    "    from loguru import logger\n",
    "    val_metrics_4tb = aggregate_metrics(metrics, 5e-4)\n",
    "    val_metrics_4tb[\"AP50\"] = recall_image / all_image\n",
    "    logger.info('\\n' + pprint.pformat(val_metrics_4tb))\n",
    "    obj_name = int(name[2:])\n",
    "    res_table.append([f\"{id2name_dict[obj_name] }\"] + list(val_metrics_4tb.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "headers = [\"Category\"] + list(val_metrics_4tb.keys())\n",
    "all_data = np.array(res_table)[:, 1:].astype(np.float32)\n",
    "res_table.append([\"Avg\"] + all_data.mean(0).tolist())\n",
    "print(tabulate(res_table, \\\n",
    "    headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
