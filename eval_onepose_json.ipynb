{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_name: ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross']\n",
      "layer_name: ['self', 'cross']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-03 10:12:25.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpope_model_api\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m185\u001b[0m - \u001b[1mload Matcher successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pope_model_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-03 10:12:32.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mload SAM model from weights/sam_vit_h_4b8939.pth\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt, model_type = get_model_info(\"h\")\n",
    "sam = sam_model_registry[model_type](checkpoint=ckpt)\n",
    "DEVICE = \"cuda\"\n",
    "sam.to(device=DEVICE)\n",
    "MASK_GEN = SamAutomaticMaskGenerator(sam)\n",
    "logger.info(f\"load SAM model from {ckpt}\")\n",
    "crop_tool = CropImage()\n",
    "dinov2_model = load_dinov2_model()\n",
    "dinov2_model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "metrics.update({'R_errs':[], 't_errs':[], 'inliers':[] , \"identifiers\":[]})\n",
    "\n",
    "# load dataset from 'data/openpose' or /data/onepose_plusplus\n",
    "ROOT_DIR = \"data/onepose/\"\n",
    "res_table = []\n",
    "\n",
    "import json\n",
    "with open(\"data/pairs/Onepose-test.json\") as f:\n",
    "    dir_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onepose: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 53/66 [05:18<01:18,  6.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m image_h, image_w, _ \u001b[38;5;241m=\u001b[39m image1\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     30\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 31\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mMASK_GEN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m similarity_score, top_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mfloat32), [[], [], []]\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/automatic_mask_generator.py:236\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    234\u001b[0m cropped_im \u001b[38;5;241m=\u001b[39m image[y0:y1, x0:x1, :]\n\u001b[1;32m    235\u001b[0m cropped_im_size \u001b[38;5;241m=\u001b[39m cropped_im\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_im\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Get points for this crop\u001b[39;00m\n\u001b[1;32m    239\u001b[0m points_scale \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cropped_im_size)[\u001b[38;5;28;01mNone\u001b[39;00m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/predictor.py:62\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     59\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(input_image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     60\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m input_image_torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/predictor.py:91\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     90\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_image_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/modeling/image_encoder.py:173\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    170\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    171\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home2/mingxintan/envs/tmx_pope/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/modeling/image_encoder.py:233\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 233\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    236\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/modeling/image_encoder.py:348\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    346\u001b[0m q_h, q_w \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    347\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 348\u001b[0m Rh \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    351\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/home2/mingxintan/POPE/segment_anything/segment_anything/modeling/image_encoder.py:321\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    318\u001b[0m k_coords \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(k_size)[\u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size \u001b[38;5;241m/\u001b[39m k_size, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    319\u001b[0m relative_coords \u001b[38;5;241m=\u001b[39m (q_coords \u001b[38;5;241m-\u001b[39m k_coords) \u001b[38;5;241m+\u001b[39m (k_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size \u001b[38;5;241m/\u001b[39m k_size, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrel_pos_resized\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelative_coords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for label_idx, test_dict in enumerate(dir_list):\n",
    "    print(f\"Onepose: {label_idx}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    sample_data = dir_list[label_idx][\"0\"][0]\n",
    "    label = sample_data.split(\"/\")[0]\n",
    "    name = label.split(\"-\")[1]\n",
    "    dir_name = os.path.dirname(sample_data)\n",
    "    FULL_ROOT_DIR = os.path.join(ROOT_DIR, dir_name)\n",
    "    recall_image, all_image = 0, 0\n",
    "    for rotation_key, rotation_list in zip(test_dict.keys(), test_dict.values()):\n",
    "        for pair_idx, pair_name in enumerate(tqdm(rotation_list)):\n",
    "            all_image = all_image + 1\n",
    "            base_name = os.path.basename(pair_name)\n",
    "            idx0_name = base_name.split(\"-\")[0]\n",
    "            idx1_name = base_name.split(\"-\")[1]\n",
    "            image0_name = os.path.join(FULL_ROOT_DIR, idx0_name)\n",
    "\n",
    "            image1_name = os.path.join(FULL_ROOT_DIR.replace(\"color\", \"color\"), idx1_name)\n",
    "            intrinsic_path = image0_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "            K0 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "            intrinsic_path = image1_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "            K1 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "\n",
    "            image0 = cv2.imread(image0_name)\n",
    "            ref_torch_image = set_torch_image(image0, center_crop=True).to('cuda:1')\n",
    "            ref_fea = get_cls_token_torch(dinov2_model, ref_torch_image)\n",
    "            image1 = cv2.imread(image1_name)\n",
    "            image_h, image_w, _ = image1.shape\n",
    "            t1 = time.time()\n",
    "            masks = MASK_GEN.generate(image1)\n",
    "            t2 = time.time()\n",
    "            similarity_score, top_images = np.array([0, 0, 0], np.float32), [[], [], []]\n",
    "            t3 = time.time()\n",
    "            compact_percent = 0.3\n",
    "            for xxx, mask in enumerate(masks):\n",
    "                object_mask = np.expand_dims(mask[\"segmentation\"], -1)\n",
    "                x0, y0, w, h = mask[\"bbox\"]\n",
    "                x1, y1 = x0 + w, y0 + h\n",
    "                x0 -= int(w * compact_percent)\n",
    "                y0 -= int(h * compact_percent)\n",
    "                x1 += int(w * compact_percent)\n",
    "                y1 += int(h * compact_percent)\n",
    "                box = np.array([x0, y0, x1, y1])\n",
    "                resize_shape = np.array([y1 - y0, x1 - x0])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box, K1, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image1, box, resize_shape)\n",
    "                # object_mask,_ = get_image_crop_resize(object_mask, box, resize_shape)\n",
    "                box_new = np.array([0, 0, x1 - x0, y1 - y0])\n",
    "                resize_shape = np.array([512, 512])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box_new, K_crop, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image_crop, box_new, resize_shape)\n",
    "                crop_tensor = set_torch_image(image_crop, center_crop=True).to('cuda:1')\n",
    "                with torch.no_grad():\n",
    "                    fea = get_cls_token_torch(dinov2_model, crop_tensor)\n",
    "                score = F.cosine_similarity(ref_fea, fea, dim=1, eps=1e-8)\n",
    "                if (score.item() > similarity_score).any():\n",
    "                    mask[\"crop_image\"] = image_crop\n",
    "                    mask[\"K\"] = K_crop\n",
    "                    mask[\"bbox\"] = box\n",
    "                    min_idx = np.argmin(similarity_score)\n",
    "                    similarity_score[min_idx] = score.item()\n",
    "                    top_images[min_idx] = mask.copy()\n",
    "\n",
    "            img0 = cv2.cvtColor(image0, cv2.COLOR_BGR2GRAY)\n",
    "            img0 = torch.from_numpy(img0).float()[None] / 255.\n",
    "            img0 = img0.unsqueeze(0).to('cuda:1')\n",
    "\n",
    "            matching_score = [[0] for _ in range(len(top_images))]\n",
    "            for top_idx in range(len(top_images)):\n",
    "                if \"crop_image\" not in top_images[top_idx]:\n",
    "                    continue\n",
    "                img1 = cv2.cvtColor(top_images[top_idx][\"crop_image\"], cv2.COLOR_BGR2GRAY)\n",
    "                img1 = torch.from_numpy(img1).float()[None] / 255.\n",
    "                img1 = img1.unsqueeze(0).to('cuda:1')\n",
    "                batch = {'image0':img0, 'image1':img1}\n",
    "                with torch.no_grad():\n",
    "                    matcher(batch)\n",
    "                    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "                    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "                    confidences = batch[\"mconf\"].cpu().numpy()\n",
    "                conf_mask = np.where(confidences > 0.9)\n",
    "                matching_score[top_idx] = conf_mask[0].shape[0]\n",
    "                top_images[top_idx][\"mkpts0\"] = mkpts0\n",
    "                top_images[top_idx][\"mkpts1\"] = mkpts1\n",
    "                top_images[top_idx][\"mconf\"] = confidences\n",
    "            #---------------------------------------------------\n",
    "            # crop_image = cv2.resize(top_images[np.argmax(matching_score)][\"crop_image\"],(256,256))\n",
    "            # que_image = cv2.resize(image0,(256,256))\n",
    "            # image = np.hstack((que_image, crop_image))\n",
    "            # for top_idx in range(len(top_images)):\n",
    "            #     crop_image = top_images[top_idx][\"crop_image\"]\n",
    "            #     score = matching_score[top_idx]\n",
    "            #     crop_image = cv2.resize(crop_image,(256,256))\n",
    "            #     cv2.putText(crop_image,f'{score}',(100,100),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),1)\n",
    "            #     image = np.hstack((image, crop_image))\n",
    "            # cv2.imwrite(f\"segment_anything/crop_images/{idx}.jpg\", image)\n",
    "            #---------------------------------------------------\n",
    "            t4 = time.time()\n",
    "            # print(f\"t4-t3: object detection:{1000*(t4-t3)} ms\")\n",
    "            pose0_name = image0_name.replace(\"color\", \"poses_ba\").replace(\"png\",\"txt\")\n",
    "            pose1_name = image1_name.replace(\"color\", \"poses_ba\").replace(\"png\",\"txt\")\n",
    "            pose0 = np.loadtxt(pose0_name)\n",
    "            pose1 = np.loadtxt(pose1_name)\n",
    "            # pose0 = np.vstack((pose0, np.array([[0,0,0,1]])))\n",
    "            # pose1 = np.vstack((pose1, np.array([[0,0,0,1]])))\n",
    "            relative_pose =  np.matmul(pose1, inv(pose0))\n",
    "            t = relative_pose[:3, -1].reshape(1, 3)\n",
    "            if \"crop_image\" not in top_images[top_idx]:\n",
    "                continue\n",
    "            max_match_idx = np.argmax(matching_score)\n",
    "            pre_bbox  = top_images[max_match_idx][\"bbox\"]\n",
    "            mkpts0 = top_images[max_match_idx][\"mkpts0\"]\n",
    "            mkpts1 = top_images[max_match_idx][\"mkpts1\"]\n",
    "            pre_K = top_images[max_match_idx][\"K\"]\n",
    "\n",
    "            _3d_bbox = np.loadtxt(f\"{os.path.join(ROOT_DIR, label)}/box3d_corners.txt\")\n",
    "            bbox_pts_3d, _ = project_points(_3d_bbox, pose1[:3, :4], K1)\n",
    "            bbox_pts_3d = bbox_pts_3d.astype(np.int32)\n",
    "            x0, y0, w, h = cv2.boundingRect(bbox_pts_3d)\n",
    "            x1, y1 = x0 + w, y0 + h\n",
    "            gt_bbox = np.array([x0, y0, x1, y1])\n",
    "            is_recalled = recall_object(pre_bbox , gt_bbox)\n",
    "            recall_image = recall_image + int(is_recalled > 0.5)\n",
    "            ret = estimate_pose(mkpts0, mkpts1, K0, pre_K, 0.5, 0.99)\n",
    "            if ret is  not None:\n",
    "                Rot, t, inliers = ret\n",
    "                t_err, R_err = relative_pose_error(relative_pose, Rot, t, ignore_gt_t_thr=0.0)\n",
    "                metrics['R_errs'].append(R_err)\n",
    "                metrics['t_errs'].append(t_err)\n",
    "            else:\n",
    "                metrics['R_errs'].append(90)\n",
    "                metrics['t_errs'].append(90)\n",
    "            metrics[\"identifiers\"].append(pair_name)\n",
    "\n",
    "\n",
    "    print(f\"Acc: {recall_image}/{all_image}\")\n",
    "    import pprint\n",
    "    from src.utils.metrics import (\n",
    "        aggregate_metrics\n",
    "    )\n",
    "    from loguru import logger\n",
    "    val_metrics_4tb = aggregate_metrics(metrics, 5e-4)\n",
    "    val_metrics_4tb[\"AP50\"] = recall_image/all_image\n",
    "    logger.info('\\n' + pprint.pformat(val_metrics_4tb))\n",
    "\n",
    "    res_table.append([f\"{name }\"] + list(val_metrics_4tb.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════════╤══════════╕\n",
      "│ Category       │   R:auc@15 │   R:auc@30 │   R:ACC15 │   R:ACC30 │   R:medianErr │   t:auc@15 │   t:auc@30 │   t:ACC15 │   t:ACC30 │   t:medianErr │     AP50 │\n",
      "╞════════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════════╪══════════╡\n",
      "│ aptamil        │   0.671279 │   0.79792  │  0.894737 │  0.94508  │       2.89285 │   0.468633 │   0.62122  │  0.707094 │  0.819222 │       6.53454 │ 0.915718 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ jzhg           │   0.824372 │   0.897068 │  0.955782 │  0.97619  │       1.50379 │   0.696935 │   0.81914  │  0.908163 │  0.952381 │       2.74556 │ 0.911565 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ minipuff       │   0.773874 │   0.862028 │  0.929515 │  0.982379 │       1.51574 │   0.675029 │   0.797644 │  0.876652 │  0.942731 │       3.01591 │ 0.898678 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ hlyormosiapie  │   0.70592  │   0.81921  │  0.875912 │  0.978102 │       2.23773 │   0.571555 │   0.700859 │  0.784672 │  0.857664 │       4.46292 │ 0.956204 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ brownhouse     │   0.831933 │   0.903085 │  0.96     │  0.988    │       1.59955 │   0.725833 │   0.843846 │  0.948    │  0.968    │       2.5116  │ 0.796    │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ oreo           │   0.834392 │   0.903607 │  0.953901 │  0.985816 │       1.37979 │   0.68406  │   0.799747 │  0.890071 │  0.932624 │       3.0276  │ 0.914894 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ mfmilkcake     │   0.78318  │   0.871988 │  0.934066 │  0.975275 │       1.63324 │   0.695464 │   0.802217 │  0.884615 │  0.923077 │       2.53043 │ 0.906593 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ diycookies     │   0.660987 │   0.766803 │  0.813115 │  0.911475 │       2.54309 │   0.511475 │   0.644659 │  0.737705 │  0.806557 │       5.3683  │ 0.845902 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ taipingcookies │   0.771827 │   0.876042 │  0.954128 │  0.987768 │       2.15812 │   0.684414 │   0.806345 │  0.896024 │  0.941896 │       3.18978 │ 0.556575 │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ tee            │   0.616064 │   0.754408 │  0.846535 │  0.925743 │       3.75194 │   0.419815 │   0.587618 │  0.673267 │  0.804455 │       7.68807 │ 0.75     │\n",
      "├────────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ Avg            │   0.747383 │   0.845216 │  0.911769 │  0.965583 │       2.12158 │   0.613321 │   0.74233  │  0.830626 │  0.894861 │       4.10747 │ 0.845213 │\n",
      "╘════════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "headers = [\"Category\"] + list(val_metrics_4tb.keys())\n",
    "all_data = np.array(res_table)[:, 1:].astype(np.float32)\n",
    "res_table.append([\"Avg\"] + all_data.mean(0).tolist())\n",
    "print(tabulate(res_table, headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(res_table, columns=headers)\n",
    "\n",
    "df_rounded = df.round(6)\n",
    "\n",
    "df_rounded.to_excel(\"res.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
