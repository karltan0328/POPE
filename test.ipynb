{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ycbv-points/0801-1-other/mkpts0/1572-1.png-1536-1.png.txt does not exist\n",
      "data/ycbv-points/0801-1-other/mkpts0/1586-1.png-1566-1.png.txt does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from pose.dataset import pose_dataset\n",
    "from pose.utils import (\n",
    "    collate_fn,\n",
    "    geodesic_distance,\n",
    "    relative_pose_error,\n",
    "    relative_pose_error_np,\n",
    "    recall_object,\n",
    "    aggregate_metrics\n",
    ")\n",
    "from pose.model import Mkpts_Reg_Model\n",
    "from pose.animator import Animator\n",
    "\n",
    "\n",
    "if os.name == 'nt':\n",
    "    LM_dataset_path = 'd:/git_project/POPE/data/LM_dataset/'\n",
    "    LM_dataset_json_path = 'd:/git_project/POPE/data/pairs/LINEMOD-test.json'\n",
    "    LM_dataset_points_path = 'd:/git_project/POPE/data/LM_dataset-points/'\n",
    "\n",
    "    onepose_path = 'd:/git_project/POPE/data/onepose/'\n",
    "    onepose_json_path = 'd:/git_project/POPE/data/pairs/Onepose-test.json'\n",
    "    onepose_points_path = 'd:/git_project/POPE/data/onepose-points/'\n",
    "\n",
    "    onepose_plusplus_path = 'd:/git_project/POPE/data/onepose_plusplus/'\n",
    "    onepose_plusplus_json_path = 'd:/git_project/POPE/data/pairs/OneposePlusPlus-test.json'\n",
    "    onepose_plusplus_points_path = 'd:/git_project/POPE/data/onepose_plusplus-points/'\n",
    "\n",
    "    ycbv_path = 'd:/git_project/POPE/data/ycbv/'\n",
    "    ycbv_json_path = 'd:/git_project/POPE/data/pairs/YCB-VIDEO-test.json'\n",
    "    ycbv_points_path = 'd:/git_project/POPE/data/ycbv-points'\n",
    "elif os.name == 'posix':\n",
    "    LM_dataset_path = 'data/LM_dataset/'\n",
    "    LM_dataset_json_path = 'data/pairs/LINEMOD-test.json'\n",
    "    LM_dataset_points_path = 'data/LM_dataset-points/'\n",
    "\n",
    "    onepose_path = 'data/onepose/'\n",
    "    onepose_json_path = 'data/pairs/Onepose-test.json'\n",
    "    onepose_points_path = 'data/onepose-points/'\n",
    "\n",
    "    onepose_plusplus_path = 'data/onepose_plusplus/'\n",
    "    onepose_plusplus_json_path = 'data/pairs/OneposePlusPlus-test.json'\n",
    "    onepose_plusplus_points_path = 'data/onepose_plusplus-points/'\n",
    "\n",
    "    ycbv_path = 'data/ycbv/'\n",
    "    ycbv_json_path = 'data/pairs/YCB-VIDEO-test.json'\n",
    "    ycbv_points_path = 'data/ycbv-points'\n",
    "\n",
    "paths = [\n",
    "    # ('linemod', LM_dataset_path, LM_dataset_json_path, LM_dataset_points_path),\n",
    "    # ('onepose', onepose_path, onepose_json_path, onepose_points_path),\n",
    "    # ('onepose_plusplus', onepose_plusplus_path, onepose_plusplus_json_path, onepose_plusplus_points_path),\n",
    "    ('ycbv', ycbv_path, ycbv_json_path, ycbv_points_path),\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = pose_dataset(paths)\n",
    "mkpts_max_len, mkpts_sum_len = dataset.get_mkpts_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20231223)\n",
    "torch.manual_seed(20231223)\n",
    "torch.cuda.manual_seed(20231223)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 300\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_fn(num_sample))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_fn(num_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mkpts_Reg_Model(\n",
       "  (embedding): Embedding()\n",
       "  (transformer_mkpts): Transformer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=76, out_features=76, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
       "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): Linear(in_features=22800, out_features=11400, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=11400, out_features=2000, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (convnextv2): ConvNeXtV2(\n",
       "    (model): ConvNeXtV2(\n",
       "      (downsample_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(3, 352, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (1): LayerNorm()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): Conv2d(352, 704, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): Conv2d(704, 1408, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): Conv2d(1408, 2816, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (stages): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(352, 352, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=352)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=352, out_features=1408, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=1408, out_features=352, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(352, 352, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=352)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=352, out_features=1408, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=1408, out_features=352, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(352, 352, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=352)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=352, out_features=1408, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=1408, out_features=352, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=704)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=704, out_features=2816, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=2816, out_features=704, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=704)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=704, out_features=2816, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=2816, out_features=704, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=704)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=704, out_features=2816, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=2816, out_features=704, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (9): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (10): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (11): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (12): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (13): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (14): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (15): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (16): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (17): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (18): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (19): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (20): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (21): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (22): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (23): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (24): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (25): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (26): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(2816, 2816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2816)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(2816, 2816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2816)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(2816, 2816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2816)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((2816,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Linear(in_features=2816, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mkpts_as_q): Transformer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1000, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (cnn_as_q): Transformer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1000, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mlp2): Sequential(\n",
       "    (0): Linear(in_features=4000, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): Dropout(p=0.1, inplace=False)\n",
       "    (15): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (16): LeakyReLU(negative_slope=0.01)\n",
       "    (17): Dropout(p=0.1, inplace=False)\n",
       "    (18): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (19): LeakyReLU(negative_slope=0.01)\n",
       "    (20): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (translation_head): Linear(in_features=32, out_features=3, bias=True)\n",
       "  (rotation_head): Linear(in_features=32, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.load('./weights/20240505/ycbv-mkpts-imgs-relative_r-gt_t-6d-300-2024-05-04-19-36-56-5580.2383.pth').to(device)\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name_dict = {\n",
    "    1: \"ape\",\n",
    "    2: \"benchvise\",\n",
    "    4: \"camera\",\n",
    "    5: \"can\",\n",
    "    6: \"cat\",\n",
    "    8: \"driller\",\n",
    "    9: \"duck\",\n",
    "    10: \"eggbox\",\n",
    "    11: \"glue\",\n",
    "    12: \"holepuncher\",\n",
    "    13: \"iron\",\n",
    "    14: \"lamp\",\n",
    "    15: \"phone\",\n",
    "}\n",
    "\n",
    "ycbv_dict = {\n",
    "    1: 'one',\n",
    "    2: 'two',\n",
    "    3: 'three',\n",
    "    4: 'four',\n",
    "    5: 'five',\n",
    "    6: 'six',\n",
    "    7: 'seven',\n",
    "    8: 'eight',\n",
    "    9: 'nine',\n",
    "    10: 'ten',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linemod\n",
    "ape_data = []\n",
    "benchvise_data = []\n",
    "camera_data = []\n",
    "can_data = []\n",
    "cat_data = []\n",
    "driller_data = []\n",
    "duck_data = []\n",
    "eggbox_data = []\n",
    "glue_data = []\n",
    "holepuncher_data = []\n",
    "iron_data = []\n",
    "lamp_data = []\n",
    "phone_data = []\n",
    "# onepose\n",
    "aptamil_data = []\n",
    "jzhg_data = []\n",
    "minipuff_data = []\n",
    "hlyormosiapie_data = []\n",
    "brownhouse_data = []\n",
    "oreo_data = []\n",
    "mfmilkcake_data = []\n",
    "diycookies_data = []\n",
    "taipingcookies_data = []\n",
    "tee_data = []\n",
    "# onepose++\n",
    "toyrobot_data = []\n",
    "yellowduck_data = []\n",
    "sheep_data = []\n",
    "fakebanana_data = []\n",
    "teabox_data = []\n",
    "orange_data = []\n",
    "greenteapot_data = []\n",
    "lecreusetcup_data = []\n",
    "insta_data = []\n",
    "# ycbv\n",
    "one_data = []\n",
    "two_data = []\n",
    "three_data = []\n",
    "four_data = []\n",
    "five_data = []\n",
    "six_data = []\n",
    "seven_data = []\n",
    "eight_data = []\n",
    "nine_data = []\n",
    "ten_data = []\n",
    "\n",
    "all_data = {\n",
    "    # linemod\n",
    "    'ape_data': ape_data,\n",
    "    'benchvise_data': benchvise_data,\n",
    "    'camera_data': camera_data,\n",
    "    'can_data': can_data,\n",
    "    'cat_data': cat_data,\n",
    "    'driller_data': driller_data,\n",
    "    'duck_data': duck_data,\n",
    "    'eggbox_data': eggbox_data,\n",
    "    'glue_data': glue_data,\n",
    "    'holepuncher_data': holepuncher_data,\n",
    "    'iron_data': iron_data,\n",
    "    'lamp_data': lamp_data,\n",
    "    'phone_data': phone_data,\n",
    "    # onepose\n",
    "    'aptamil_data': aptamil_data,\n",
    "    'jzhg_data': jzhg_data,\n",
    "    'minipuff_data': minipuff_data,\n",
    "    'hlyormosiapie_data': hlyormosiapie_data,\n",
    "    'brownhouse_data': brownhouse_data,\n",
    "    'oreo_data': oreo_data,\n",
    "    'mfmilkcake_data': mfmilkcake_data,\n",
    "    'diycookies_data': diycookies_data,\n",
    "    'taipingcookies_data': taipingcookies_data,\n",
    "    'tee_data': tee_data,\n",
    "    # onepose++\n",
    "    'toyrobot_data': toyrobot_data,\n",
    "    'yellowduck_data': yellowduck_data,\n",
    "    'sheep_data': sheep_data,\n",
    "    'fakebanana_data': fakebanana_data,\n",
    "    'teabox_data': teabox_data,\n",
    "    'orange_data': orange_data,\n",
    "    'greenteapot_data': greenteapot_data,\n",
    "    'lecreusetcup_data': lecreusetcup_data,\n",
    "    'insta_data': insta_data,\n",
    "    # ycbv\n",
    "    'one_data': one_data,\n",
    "    'two_data': two_data,\n",
    "    'three_data': three_data,\n",
    "    'four_data': four_data,\n",
    "    'five_data': five_data,\n",
    "    'six_data': six_data,\n",
    "    'seven_data': seven_data,\n",
    "    'eight_data': eight_data,\n",
    "    'nine_data': nine_data,\n",
    "    'ten_data': ten_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linemod_type = ['ape_data', 'benchvise_data', 'camera_data', 'can_data', 'cat_data', 'driller_data', 'duck_data', 'eggbox_data', 'glue_data', 'holepuncher_data', 'iron_data', 'lamp_data', 'phone_data']\n",
    "onepose_type = ['aptamil_data', 'jzhg_data', 'minipuff_data', 'hlyormosiapie_data', 'brownhouse_data', 'oreo_data', 'mfmilkcake_data', 'diycookies_data', 'taipingcookies_data', 'tee_data']\n",
    "oneposeplusplus_type = ['toyrobot_data', 'yellowduck_data', 'sheep_data', 'fakebanana_data', 'teabox_data', 'orange_data', 'greenteapot_data', 'lecreusetcup_data', 'insta_data']\n",
    "ycbv_type = ['one_data', 'two_data', 'three_data', 'four_data', 'five_data', 'six_data', 'seven_data', 'eight_data', 'nine_data', 'ten_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_data 41\n",
      "two_data 28\n",
      "three_data 54\n",
      "four_data 31\n",
      "five_data 28\n",
      "six_data 31\n",
      "seven_data 4\n",
      "eight_data 17\n",
      "nine_data 21\n",
      "ten_data 25\n",
      "len(all_data): 10\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_dataloader):\n",
    "    for data in batch:\n",
    "        if 'lm' in data['name']:\n",
    "            all_data[f\"{id2name_dict[int(data['name'][2:])]}_data\"].append(data)\n",
    "        else:\n",
    "            if data['name'] in ('12345678910'):\n",
    "                all_data[f\"{ycbv_dict[int(data['name'])]}_data\"].append(data)\n",
    "            else:\n",
    "                all_data[f\"{data['name']}_data\"].append(data)\n",
    "\n",
    "empty_keys = []\n",
    "for key in all_data.keys():\n",
    "    if len(all_data[key]) == 0:\n",
    "        empty_keys.append(key)\n",
    "\n",
    "for key in empty_keys:\n",
    "    all_data.pop(key)\n",
    "\n",
    "for key in all_data.keys():\n",
    "    print(key, len(all_data[key]))\n",
    "\n",
    "print('len(all_data):', len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:06.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: one_data\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:08.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 41 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:08.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: two_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 18/41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:08.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 28 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:08.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: three_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 15/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:10.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 54 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:10.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: four_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 13/54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:10.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 31 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:10.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: five_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 8/31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:11.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 28 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:11.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: six_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 18/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:12.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 31 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:12.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: seven_data\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:12.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 4 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:12.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: eight_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 9/31\n",
      "Acc: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:12.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 17 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:12.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: nine_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 6/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:13.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 21 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-11 16:05:13.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: ten_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 10/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 16:05:14.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 25 unique items...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 18/25\n"
     ]
    }
   ],
   "source": [
    "res_table = []\n",
    "\n",
    "model_type = 'relative_r-gt_t'\n",
    "\n",
    "for key in all_data.keys():\n",
    "    if key in linemod_type:\n",
    "        logger.info(f\"LINEMOD: {key}\")\n",
    "    elif key in onepose_type:\n",
    "        logger.info(f\"ONEPOSE: {key}\")\n",
    "    elif key in oneposeplusplus_type:\n",
    "        logger.info(f\"ONEPOSE++: {key}\")\n",
    "    elif key in ycbv_type:\n",
    "        logger.info(f\"YCBV: {key}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    recall_image, all_image = 0, 0\n",
    "    for item in all_data[key]:\n",
    "        all_image += 1\n",
    "        K0 = item['K0']\n",
    "        K1 = item['K1']\n",
    "        pose0 = item['pose0']\n",
    "        pose1 = item['pose1']\n",
    "        pre_bbox = item['pre_bbox']\n",
    "        gt_bbox = item['gt_bbox']\n",
    "        mkpts0 = item['mkpts0']\n",
    "        mkpts1 = item['mkpts1']\n",
    "        pre_K = item['pre_K']\n",
    "        img0 = item['img0']\n",
    "        img1 = item['img1']\n",
    "        name = item['name']\n",
    "        pair_name = item['pair_name']\n",
    "        # linemod\n",
    "        if 'lm' in name:\n",
    "            name = id2name_dict[int(name[2:])]\n",
    "        # ycbv\n",
    "        if name in '12345678910':\n",
    "            name = ycbv_dict[int(name)]\n",
    "\n",
    "        if name not in key:\n",
    "            print(f'name: {name}, key: {key}')\n",
    "            continue\n",
    "\n",
    "        is_recalled = recall_object(pre_bbox, gt_bbox)\n",
    "\n",
    "        recall_image = recall_image + int(is_recalled > 0.5)\n",
    "\n",
    "        batch_mkpts0 = torch.from_numpy(mkpts0).unsqueeze(0).float().to(device)\n",
    "        batch_mkpts1 = torch.from_numpy(mkpts1).unsqueeze(0).float().to(device)\n",
    "        img0 = torch.from_numpy(img0).unsqueeze(0).float().to(device)\n",
    "        img1 = torch.from_numpy(img1).unsqueeze(0).float().to(device)\n",
    "        img0 = img0.permute(0, 3, 2, 1)\n",
    "        img1 = img1.permute(0, 3, 2, 1)\n",
    "        pre_t, pre_rot = net(batch_mkpts0, batch_mkpts1, img0, img1)\n",
    "        pre_t = pre_t.squeeze(0).detach().cpu().numpy()\n",
    "        pre_rot = pre_rot.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        if model_type == 'gt':\n",
    "            t_err, R_err = relative_pose_error_np(pose1, pre_rot, pre_t, ignore_gt_t_thr=0.0)\n",
    "        elif model_type == 'relative':\n",
    "            relative_pose = np.matmul(pose1, np.linalg.inv(pose0))\n",
    "            t_err, R_err = relative_pose_error_np(relative_pose, pre_rot, pre_t, ignore_gt_t_thr=0.0)\n",
    "        elif model_type == 'relative_r-gt_t':\n",
    "            relative_pose = np.matmul(pose1, np.linalg.inv(pose0))\n",
    "            gt_pose = np.zeros_like(pose1)\n",
    "            gt_pose[:3, :3] = relative_pose[:3, :3]\n",
    "            gt_pose[:3, 3] = pose1[:3, 3]\n",
    "            t_err, R_err = relative_pose_error_np(gt_pose, pre_rot, pre_t, ignore_gt_t_thr=0.0)\n",
    "\n",
    "        metrics['R_errs'].append(R_err)\n",
    "        metrics['t_errs'].append(t_err)\n",
    "        metrics['identifiers'].append(pair_name)\n",
    "\n",
    "    print(f\"Acc: {recall_image}/{all_image}\")\n",
    "    val_metrics_4tb = aggregate_metrics(metrics, 5e-4)\n",
    "    val_metrics_4tb[\"AP50\"] = recall_image / all_image\n",
    "    # logger.info('\\n' + pprint.pformat(val_metrics_4tb))\n",
    "    res_table.append([f\"{name}\"] + list(val_metrics_4tb.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 16:05:14\n",
      "YCBV\n",
      "╒════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════════╤═════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════════╤═════════════╤══════════╕\n",
      "│ Category   │   R:auc@ 1 │   R:auc@ 2 │   R:auc@ 3 │   R:auc@ 4 │   R:auc@ 5 │   R:auc@ 6 │   R:auc@ 7 │   R:auc@ 8 │   R:auc@ 9 │   R:auc@10 │   R:auc@11 │   R:auc@12 │   R:auc@13 │   R:auc@14 │   R:auc@15 │   R:auc@16 │   R:auc@17 │   R:auc@18 │   R:auc@19 │   R:auc@20 │   R:auc@21 │   R:auc@22 │   R:auc@23 │   R:auc@24 │   R:auc@25 │   R:auc@26 │   R:auc@27 │   R:auc@28 │   R:auc@29 │   R:auc@30 │   R:ACC 1 │   R:ACC 2 │   R:ACC 3 │   R:ACC 4 │   R:ACC 5 │   R:ACC 6 │   R:ACC 7 │   R:ACC 8 │   R:ACC 9 │   R:ACC10 │   R:ACC11 │   R:ACC12 │   R:ACC13 │   R:ACC14 │   R:ACC15 │   R:ACC16 │   R:ACC17 │   R:ACC18 │   R:ACC19 │   R:ACC20 │   R:ACC21 │   R:ACC22 │   R:ACC23 │   R:ACC24 │   R:ACC25 │   R:ACC26 │   R:ACC27 │   R:ACC28 │   R:ACC29 │   R:ACC30 │   R:medianErr │   R:meanErr │   t:auc@ 1 │   t:auc@ 2 │   t:auc@ 3 │   t:auc@ 4 │   t:auc@ 5 │   t:auc@ 6 │   t:auc@ 7 │   t:auc@ 8 │   t:auc@ 9 │   t:auc@10 │   t:auc@11 │   t:auc@12 │   t:auc@13 │   t:auc@14 │   t:auc@15 │   t:auc@16 │   t:auc@17 │   t:auc@18 │   t:auc@19 │   t:auc@20 │   t:auc@21 │   t:auc@22 │   t:auc@23 │   t:auc@24 │   t:auc@25 │   t:auc@26 │   t:auc@27 │   t:auc@28 │   t:auc@29 │   t:auc@30 │    t:ACC 1 │   t:ACC 2 │   t:ACC 3 │   t:ACC 4 │   t:ACC 5 │   t:ACC 6 │   t:ACC 7 │   t:ACC 8 │   t:ACC 9 │   t:ACC10 │   t:ACC11 │   t:ACC12 │   t:ACC13 │   t:ACC14 │   t:ACC15 │   t:ACC16 │   t:ACC17 │   t:ACC18 │   t:ACC19 │   t:ACC20 │   t:ACC21 │   t:ACC22 │   t:ACC23 │   t:ACC24 │   t:ACC25 │   t:ACC26 │   t:ACC27 │   t:ACC28 │   t:ACC29 │   t:ACC30 │   t:medianErr │   t:meanErr │     AP50 │\n",
      "╞════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════════╪═════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════════╪═════════════╪══════════╡\n",
      "│ one        │          0 │  0.0267332 │  0.0760546 │  0.12241   │  0.151586  │  0.177701  │  0.194127  │  0.211221  │   0.22599  │  0.240994  │  0.257824  │  0.272924  │   0.285701 │   0.300737 │   0.314593 │   0.32845  │   0.342957 │   0.357362 │   0.370646 │   0.382602 │   0.397283 │   0.411392 │   0.425395 │   0.439478 │   0.452143 │   0.466312 │   0.481132 │   0.496726 │   0.511557 │   0.528094 │         0 │ 0.146341  │  0.195122 │ 0.268293  │ 0.268293  │ 0.292683  │ 0.292683  │ 0.317073  │  0.341463 │  0.365854 │  0.439024 │  0.439024 │  0.439024 │  0.487805 │  0.512195 │  0.560976 │  0.585366 │  0.609756 │  0.609756 │  0.609756 │  0.682927 │  0.707317 │  0.731707 │  0.756098 │  0.756098 │  0.829268 │  0.902439 │  0.926829 │  0.926829 │  1        │      14.5239  │     14.5136 │ 0          │  0         │  0.0136709 │  0.0423837 │  0.0955923 │  0.168203  │  0.241805  │  0.306985  │   0.364517 │   0.41458  │   0.460671 │   0.503101 │   0.541324 │   0.574087 │   0.602481 │   0.627326 │   0.649248 │   0.668734 │   0.686169 │   0.701861 │   0.716058 │   0.728964 │   0.740749 │   0.751551 │   0.761489 │   0.770662 │   0.779156 │   0.787043 │   0.794387 │   0.801241 │ 0          │ 0         │ 0.0487805 │ 0.195122  │ 0.365854  │  0.634146 │ 0.707317  │  0.804878 │  0.829268 │  0.878049 │  0.926829 │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       5.35187 │     6.10552 │ 0.439024 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ two        │          0 │  0.0181085 │  0.0394729 │  0.0820774 │  0.129208  │  0.161244  │  0.184128  │  0.21134   │   0.231902 │  0.251879  │  0.271851  │  0.290748  │   0.309146 │   0.327276 │   0.343553 │   0.362783 │   0.381358 │   0.39787  │   0.412644 │   0.42594  │   0.442863 │   0.45843  │   0.475676 │   0.491571 │   0.508489 │   0.524903 │   0.542341 │   0.558686 │   0.573904 │   0.588107 │         0 │ 0.0357143 │  0.107143 │ 0.285714  │ 0.321429  │ 0.321429  │ 0.321429  │ 0.357143  │  0.392857 │  0.428571 │  0.464286 │  0.5      │  0.535714 │  0.571429 │  0.571429 │  0.678571 │  0.678571 │  0.678571 │  0.678571 │  0.678571 │  0.714286 │  0.821429 │  0.857143 │  0.857143 │  0.892857 │  0.928571 │  1        │  1        │  1        │  1        │      12.1622  │     12.8337 │ 0          │  0         │  0         │  0         │  0.0187173 │  0.0338859 │  0.0880748 │  0.18057   │   0.263181 │   0.336855 │   0.39714  │   0.447379 │   0.489888 │   0.526325 │   0.557903 │   0.585534 │   0.609914 │   0.631586 │   0.650976 │   0.668427 │   0.684216 │   0.69857  │   0.711676 │   0.723689 │   0.734742 │   0.744944 │   0.754391 │   0.763162 │   0.771329 │   0.778952 │ 0          │ 0         │ 0         │ 0         │ 0.0357143 │  0.285714 │ 0.535714  │  0.892857 │  0.928571 │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       6.7658  │     6.80048 │ 0.535714 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ three      │          0 │  0.0109372 │  0.0184689 │  0.0277148 │  0.033283  │  0.0369951 │  0.0450324 │  0.0509774 │   0.059804 │  0.0705609 │  0.0814014 │  0.0936856 │   0.110967 │   0.128173 │   0.144795 │   0.160952 │   0.17545  │   0.191163 │   0.206466 │   0.222593 │   0.23899  │   0.256205 │   0.273461 │   0.290937 │   0.308886 │   0.325496 │   0.342946 │   0.361296 │   0.38019  │   0.399294 │         0 │ 0.0185185 │  0.037037 │ 0.0555556 │ 0.0555556 │ 0.0555556 │ 0.0925926 │ 0.0925926 │  0.148148 │  0.166667 │  0.185185 │  0.277778 │  0.351852 │  0.351852 │  0.37037  │  0.407407 │  0.407407 │  0.462963 │  0.481481 │  0.537037 │  0.574074 │  0.62963  │  0.666667 │  0.703704 │  0.740741 │  0.740741 │  0.833333 │  0.888889 │  0.925926 │  0.962963 │      19.2994  │     18.3099 │ 0.0379614  │  0.0560177 │  0.0620365 │  0.0734154 │  0.0876377 │  0.1192    │  0.174715  │  0.240612  │   0.307013 │   0.369517 │   0.424126 │   0.472116 │   0.512722 │   0.547528 │   0.577692 │   0.604087 │   0.627376 │   0.648077 │   0.666599 │   0.683269 │   0.698352 │   0.712063 │   0.724582 │   0.736058 │   0.746615 │   0.756361 │   0.765385 │   0.773764 │   0.781565 │   0.788846 │ 0.0740741  │ 0.0740741 │ 0.0740741 │ 0.111111  │ 0.185185  │  0.333333 │ 0.62963   │  0.759259 │  0.888889 │  0.944444 │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       6.52671 │     6.43479 │ 0.240741 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ four       │          0 │  0         │  0         │  0.0227434 │  0.0310979 │  0.0475457 │  0.0684032 │  0.0887182 │   0.10395  │  0.12615   │  0.151547  │  0.175238  │   0.20112  │   0.228892 │   0.256643 │   0.289615 │   0.320017 │   0.347041 │   0.374726 │   0.399367 │   0.421825 │   0.44224  │   0.463345 │   0.483115 │   0.50121  │   0.519991 │   0.536574 │   0.553537 │   0.568933 │   0.583301 │         0 │ 0         │  0        │ 0.0645161 │ 0.0645161 │ 0.193548  │ 0.193548  │ 0.225806  │  0.225806 │  0.387097 │  0.419355 │  0.451613 │  0.548387 │  0.645161 │  0.645161 │  0.806452 │  0.806452 │  0.806452 │  0.83871  │  0.870968 │  0.870968 │  0.870968 │  0.903226 │  0.935484 │  0.935484 │  0.967742 │  0.967742 │  1        │  1        │  1        │      12.4158  │     12.9493 │ 0          │  0         │  0         │  0.0216085 │  0.0423593 │  0.0800911 │  0.123414  │  0.173259  │   0.218073 │   0.271368 │   0.328114 │   0.378962 │   0.421771 │   0.462712 │   0.498762 │   0.530089 │   0.557731 │   0.582301 │   0.604285 │   0.624071 │   0.641973 │   0.658247 │   0.673105 │   0.686726 │   0.699257 │   0.710824 │   0.721534 │   0.731479 │   0.740739 │   0.749381 │ 0          │ 0         │ 0         │ 0.0645161 │ 0.193548  │  0.322581 │ 0.483871  │  0.548387 │  0.612903 │  0.870968 │  0.903226 │  0.935484 │  0.935484 │  0.967742 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       7.44737 │     7.75501 │ 0.258065 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ five       │          0 │  0         │  0.0404079 │  0.0770651 │  0.113803  │  0.147247  │  0.180757  │  0.211734  │   0.243745 │  0.269371  │  0.290337  │  0.307809  │   0.331073 │   0.348241 │   0.366054 │   0.382842 │   0.398137 │   0.414847 │   0.430329 │   0.446295 │   0.460758 │   0.476316 │   0.489768 │   0.502099 │   0.513443 │   0.527147 │   0.54098  │   0.55633  │   0.571629 │   0.585908 │         0 │ 0         │  0.107143 │ 0.214286  │ 0.285714  │ 0.321429  │ 0.428571  │ 0.428571  │  0.5      │  0.5      │  0.5      │  0.5      │  0.571429 │  0.571429 │  0.607143 │  0.642857 │  0.642857 │  0.678571 │  0.714286 │  0.75     │  0.75     │  0.785714 │  0.785714 │  0.785714 │  0.785714 │  0.857143 │  0.928571 │  1        │  1        │  1        │      10.4745  │     12.9184 │ 0          │  0.0372253 │  0.0486264 │  0.077504  │  0.0977175 │  0.13027   │  0.169655  │  0.205986  │   0.234686 │   0.263351 │   0.291512 │   0.338752 │   0.389617 │   0.433216 │   0.471001 │   0.504064 │   0.533237 │   0.559168 │   0.58237  │   0.603251 │   0.622144 │   0.639319 │   0.655001 │   0.669376 │   0.682601 │   0.694809 │   0.706112 │   0.716608 │   0.72638  │   0.735501 │ 0          │ 0.0714286 │ 0.0714286 │ 0.178571  │ 0.178571  │  0.321429 │ 0.428571  │  0.464286 │  0.464286 │  0.5      │  0.571429 │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       9.73375 │     8.14203 │ 0.642857 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ six        │          0 │  0.0296664 │  0.0705345 │  0.0932235 │  0.121182  │  0.142196  │  0.158748  │  0.171163  │   0.180819 │  0.188543  │  0.203931  │  0.21113   │   0.225812 │   0.241257 │   0.258816 │   0.276469 │   0.295091 │   0.313703 │   0.331148 │   0.346849 │   0.361054 │   0.373968 │   0.390966 │   0.406896 │   0.423154 │   0.439137 │   0.455702 │   0.473034 │   0.490888 │   0.507859 │         0 │ 0.129032  │  0.16129  │ 0.16129   │ 0.225806  │ 0.258065  │ 0.258065  │ 0.258065  │  0.258065 │  0.258065 │  0.290323 │  0.290323 │  0.387097 │  0.451613 │  0.516129 │  0.548387 │  0.612903 │  0.645161 │  0.645161 │  0.645161 │  0.645161 │  0.645161 │  0.741935 │  0.774194 │  0.83871  │  0.83871  │  0.870968 │  0.935484 │  1        │  1        │      14.6715  │     15.2282 │ 0          │  0         │  0         │  0         │  0.0239561 │  0.0497642 │  0.0850615 │  0.146471  │   0.20785  │   0.2627   │   0.316493 │   0.372984 │   0.421216 │   0.462558 │   0.498387 │   0.529738 │   0.557401 │   0.58199  │   0.60399  │   0.623791 │   0.641705 │   0.657991 │   0.672861 │   0.686492 │   0.699032 │   0.710608 │   0.721326 │   0.731279 │   0.740545 │   0.749194 │ 0          │ 0         │ 0         │ 0         │ 0.16129   │  0.193548 │ 0.451613  │  0.612903 │  0.709677 │  0.806452 │  0.967742 │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       7.11021 │     7.70732 │ 0.290323 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ seven      │          0 │  0.158637  │  0.254189  │  0.391445  │  0.513156  │  0.594297  │  0.652254  │  0.695722  │   0.729531 │  0.756578  │  0.778707  │  0.797148  │   0.812752 │   0.826127 │   0.837719 │   0.847861 │   0.856811 │   0.864766 │   0.871883 │   0.878289 │   0.884085 │   0.889354 │   0.894164 │   0.898574 │   0.902631 │   0.906376 │   0.909844 │   0.913064 │   0.916061 │   0.918859 │         0 │ 0.25      │  0.5      │ 1         │ 1         │ 1         │ 1         │ 1         │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │       3.16498 │      2.9205 │ 0          │  0         │  0         │  0         │  0         │  0.126285  │  0.143958  │  0.246706  │   0.330405 │   0.397364 │   0.45215  │   0.497804 │   0.536434 │   0.569546 │   0.598243 │   0.623353 │   0.645509 │   0.665202 │   0.682823 │   0.698682 │   0.713031 │   0.726075 │   0.737985 │   0.748902 │   0.758946 │   0.768217 │   0.776802 │   0.784773 │   0.792195 │   0.799121 │ 0          │ 0         │ 0         │ 0         │ 0         │  0.25     │ 0.25      │  1        │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       7.19254 │     6.97186 │ 0.75     │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ eight      │          0 │  0.035653  │  0.0610232 │  0.0751792 │  0.101041  │  0.123417  │  0.139399  │  0.151386  │   0.160709 │  0.183289  │  0.193364  │  0.207952  │   0.219105 │   0.234715 │   0.246518 │   0.256846 │   0.272226 │   0.285638 │   0.298468 │   0.310015 │   0.320463 │   0.32996  │   0.34789  │   0.364231 │   0.38025  │   0.395037 │   0.408728 │   0.426508 │   0.443992 │   0.463102 │         0 │ 0.0588235 │  0.117647 │ 0.117647  │ 0.235294  │ 0.235294  │ 0.235294  │ 0.235294  │  0.235294 │  0.294118 │  0.294118 │  0.352941 │  0.352941 │  0.411765 │  0.411765 │  0.411765 │  0.470588 │  0.529412 │  0.529412 │  0.529412 │  0.529412 │  0.529412 │  0.705882 │  0.764706 │  0.764706 │  0.764706 │  0.764706 │  0.823529 │  0.941176 │  1        │      17.6893  │     16.9606 │ 0          │  0         │  0         │  0         │  0         │  0         │  0.0312545 │  0.0651915 │   0.127315 │   0.199012 │   0.271987 │   0.332655 │   0.383989 │   0.42799  │   0.466124 │   0.499491 │   0.528933 │   0.555103 │   0.578519 │   0.599593 │   0.61866  │   0.635993 │   0.65182  │   0.666327 │   0.679674 │   0.691994 │   0.703402 │   0.713995 │   0.723857 │   0.733062 │ 0          │ 0         │ 0         │ 0         │ 0         │  0        │ 0.0588235 │  0.470588 │  0.705882 │  0.941176 │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       8.05066 │     8.30743 │ 0.352941 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ nine       │          0 │  0.0266751 │  0.0611491 │  0.0815761 │  0.111647  │  0.140713  │  0.176676  │  0.208857  │   0.238561 │  0.271388  │  0.301771  │  0.328211  │   0.350583 │   0.369759 │   0.386378 │   0.40092  │   0.413751 │   0.436145 │   0.448278 │   0.459197 │   0.469077 │   0.484476 │   0.4973   │   0.508325 │   0.518469 │   0.533453 │   0.547206 │   0.562062 │   0.577083 │   0.591181 │         0 │ 0.047619  │  0.142857 │ 0.142857  │ 0.238095  │ 0.285714  │ 0.380952  │ 0.47619   │  0.47619  │  0.571429 │  0.619048 │  0.619048 │  0.619048 │  0.619048 │  0.619048 │  0.619048 │  0.619048 │  0.666667 │  0.666667 │  0.666667 │  0.666667 │  0.714286 │  0.761905 │  0.761905 │  0.761905 │  0.904762 │  0.904762 │  0.952381 │  1        │  1        │       9.24063 │     12.9374 │ 0          │  0.119848  │  0.216175  │  0.286792  │  0.334196  │  0.377057  │  0.413174  │  0.445528  │   0.485674 │   0.532345 │   0.57053  │   0.602351 │   0.637123 │   0.663043 │   0.685507 │   0.705163 │   0.722506 │   0.737922 │   0.751716 │   0.76413  │   0.775362 │   0.785573 │   0.794896 │   0.803442 │   0.811304 │   0.818562 │   0.825281 │   0.831521 │   0.837331 │   0.842753 │ 0          │ 0.333333  │ 0.428571  │ 0.52381   │ 0.52381   │  0.571429 │ 0.619048  │  0.666667 │  0.952381 │  0.952381 │  0.952381 │  0.952381 │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       3.7827  │     5.0223  │ 0.47619  │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ ten        │          0 │  0.0258973 │  0.0658068 │  0.101763  │  0.153549  │  0.205623  │  0.244819  │  0.281531  │   0.308027 │  0.335063  │  0.360941  │  0.38721   │   0.409732 │   0.429037 │   0.451311 │   0.468104 │   0.482921 │   0.501361 │   0.517179 │   0.53132  │   0.544114 │   0.55873  │   0.570959 │   0.582169 │   0.592483 │   0.602003 │   0.615143 │   0.627029 │   0.639891 │   0.651894 │         0 │ 0.04      │  0.16     │ 0.24      │ 0.4       │ 0.48      │ 0.48      │ 0.52      │  0.52     │  0.56     │  0.64     │  0.68     │  0.68     │  0.68     │  0.72     │  0.72     │  0.72     │  0.76     │  0.8      │  0.8      │  0.8      │  0.84     │  0.84     │  0.84     │  0.84     │  0.84     │  0.88     │  1        │  1        │  1        │       7.3015  │     11.0021 │ 0          │  0         │  0         │  0         │  0.0217692 │  0.0459903 │  0.0971026 │  0.175369  │   0.2524   │   0.326199 │   0.387454 │   0.438499 │   0.481692 │   0.518714 │   0.5508   │   0.578875 │   0.603647 │   0.625666 │   0.645368 │   0.6631   │   0.679143 │   0.693727 │   0.707043 │   0.71925  │   0.73048  │   0.740846 │   0.750444 │   0.759357 │   0.767655 │   0.7754   │ 0          │ 0         │ 0         │ 0         │ 0.04      │  0.32     │ 0.52      │  0.8      │  0.92     │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       6.86908 │     6.92177 │ 0.72     │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ Avg        │          0 │  0.0332308 │  0.0687107 │  0.10752   │  0.145955  │  0.177698  │  0.204435  │  0.228265  │   0.248304 │  0.269382  │  0.289167  │  0.307205  │   0.325599 │   0.343421 │   0.360638 │   0.377484 │   0.393872 │   0.41099  │   0.426177 │   0.440247 │   0.454051 │   0.468107 │   0.482892 │   0.49674  │   0.510116 │   0.523985 │   0.538059 │   0.552827 │   0.567413 │   0.58176  │         0 │ 0.0726049 │  0.152824 │ 0.255016  │ 0.30947   │ 0.344372  │ 0.368313  │ 0.391074  │  0.409782 │  0.45318  │  0.485134 │  0.511073 │  0.548549 │  0.57901  │  0.597324 │  0.639546 │  0.654319 │  0.683755 │  0.696404 │  0.708757 │  0.723349 │  0.754392 │  0.799418 │  0.817895 │  0.831621 │  0.867164 │  0.905252 │  0.952711 │  0.979393 │  0.996296 │      12.0944  │     13.0574 │ 0.00379614 │  0.0213091 │  0.0340509 │  0.0501704 │  0.0721945 │  0.113075  │  0.156821  │  0.218668  │   0.279111 │   0.337329 │   0.390018 │   0.43846  │   0.481578 │   0.518572 │   0.55069  │   0.578772 │   0.60355  │   0.625575 │   0.645282 │   0.663017 │   0.679064 │   0.693652 │   0.706972 │   0.719181 │   0.730414 │   0.740783 │   0.750383 │   0.759298 │   0.767598 │   0.775345 │ 0.00740741 │ 0.0478836 │ 0.0622855 │ 0.107313  │ 0.168397  │  0.323218 │ 0.468459  │  0.701983 │  0.801186 │  0.889347 │  0.932161 │  0.988787 │  0.993548 │  0.996774 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       6.88307 │     7.01685 │ 0.470586 │\n",
      "╘════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════════╧═════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════════╧═════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for i, key in enumerate(all_data.keys()):\n",
    "    if i == 1:\n",
    "        break\n",
    "    if key in linemod_type:\n",
    "        print(f\"LINEMOD\")\n",
    "    elif key in onepose_type:\n",
    "        print(f\"ONEPOSE\")\n",
    "    elif key in oneposeplusplus_type:\n",
    "        print(f\"ONEPOSE++\")\n",
    "    elif key in ycbv_type:\n",
    "        print(f\"YCBV\")\n",
    "\n",
    "from tabulate import tabulate\n",
    "headers = [\"Category\"] + list(val_metrics_4tb.keys())\n",
    "all_data = np.array(res_table)[:, 1:].astype(np.float32)\n",
    "res_table.append([\"Avg\"] + all_data.mean(0).tolist())\n",
    "print(tabulate(res_table, headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(res_table, columns=headers)\n",
    "\n",
    "df_rounded = df.round(6)\n",
    "\n",
    "df_rounded.to_excel(\"res.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
