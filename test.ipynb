{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ycbv-points/0801-1-other/mkpts0/1572-1.png-1536-1.png.txt does not exist\n",
      "data/ycbv-points/0801-1-other/mkpts0/1586-1.png-1566-1.png.txt does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from pose.dataset import pose_dataset\n",
    "from pose.utils import (\n",
    "    collate_fn,\n",
    "    geodesic_distance,\n",
    "    relative_pose_error,\n",
    "    relative_pose_error_np,\n",
    "    recall_object,\n",
    "    aggregate_metrics\n",
    ")\n",
    "from pose.model import Mkpts_Reg_Model\n",
    "from pose.animator import Animator\n",
    "\n",
    "\n",
    "if os.name == 'nt':\n",
    "    LM_dataset_path = 'd:/git_project/POPE/data/LM_dataset/'\n",
    "    LM_dataset_json_path = 'd:/git_project/POPE/data/pairs/LINEMOD-test.json'\n",
    "    LM_dataset_points_path = 'd:/git_project/POPE/data/LM_dataset-points/'\n",
    "\n",
    "    onepose_path = 'd:/git_project/POPE/data/onepose/'\n",
    "    onepose_json_path = 'd:/git_project/POPE/data/pairs/Onepose-test.json'\n",
    "    onepose_points_path = 'd:/git_project/POPE/data/onepose-points/'\n",
    "\n",
    "    onepose_plusplus_path = 'd:/git_project/POPE/data/onepose_plusplus/'\n",
    "    onepose_plusplus_json_path = 'd:/git_project/POPE/data/pairs/OneposePlusPlus-test.json'\n",
    "    onepose_plusplus_points_path = 'd:/git_project/POPE/data/onepose_plusplus-points/'\n",
    "\n",
    "    ycbv_path = 'd:/git_project/POPE/data/ycbv/'\n",
    "    ycbv_json_path = 'd:/git_project/POPE/data/pairs/YCB-VIDEO-test.json'\n",
    "    ycbv_points_path = 'd:/git_project/POPE/data/ycbv-points'\n",
    "elif os.name == 'posix':\n",
    "    LM_dataset_path = 'data/LM_dataset/'\n",
    "    LM_dataset_json_path = 'data/pairs/LINEMOD-test.json'\n",
    "    LM_dataset_points_path = 'data/LM_dataset-points/'\n",
    "\n",
    "    onepose_path = 'data/onepose/'\n",
    "    onepose_json_path = 'data/pairs/Onepose-test.json'\n",
    "    onepose_points_path = 'data/onepose-points/'\n",
    "\n",
    "    onepose_plusplus_path = 'data/onepose_plusplus/'\n",
    "    onepose_plusplus_json_path = 'data/pairs/OneposePlusPlus-test.json'\n",
    "    onepose_plusplus_points_path = 'data/onepose_plusplus-points/'\n",
    "\n",
    "    ycbv_path = 'data/ycbv/'\n",
    "    ycbv_json_path = 'data/pairs/YCB-VIDEO-test.json'\n",
    "    ycbv_points_path = 'data/ycbv-points'\n",
    "\n",
    "paths = [\n",
    "    # ('linemod', LM_dataset_path, LM_dataset_json_path, LM_dataset_points_path),\n",
    "    # ('onepose', onepose_path, onepose_json_path, onepose_points_path),\n",
    "    # ('onepose_plusplus', onepose_plusplus_path, onepose_plusplus_json_path, onepose_plusplus_points_path),\n",
    "    ('ycbv', ycbv_path, ycbv_json_path, ycbv_points_path),\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = pose_dataset(paths)\n",
    "mkpts_max_len, mkpts_sum_len = dataset.get_mkpts_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20231223)\n",
    "torch.manual_seed(20231223)\n",
    "torch.cuda.manual_seed(20231223)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 300\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_fn(num_sample))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_fn(num_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mkpts_Reg_Model(\n",
       "  (embedding): Embedding()\n",
       "  (transformer_mkpts): Transformer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=76, out_features=76, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
       "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): Linear(in_features=22800, out_features=11400, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=11400, out_features=2000, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (convnextv2): ConvNeXtV2(\n",
       "    (model): ConvNeXtV2(\n",
       "      (downsample_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(3, 352, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (1): LayerNorm()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): Conv2d(352, 704, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): Conv2d(704, 1408, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): LayerNorm()\n",
       "          (1): Conv2d(1408, 2816, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (stages): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(352, 352, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=352)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=352, out_features=1408, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=1408, out_features=352, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(352, 352, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=352)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=352, out_features=1408, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=1408, out_features=352, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(352, 352, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=352)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=352, out_features=1408, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=1408, out_features=352, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=704)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=704, out_features=2816, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=2816, out_features=704, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=704)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=704, out_features=2816, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=2816, out_features=704, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=704)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=704, out_features=2816, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=2816, out_features=704, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (9): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (10): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (11): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (12): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (13): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (14): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (15): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (16): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (17): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (18): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (19): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (20): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (21): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (22): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (23): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (24): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (25): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (26): Block(\n",
       "            (dwconv): Conv2d(1408, 1408, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1408)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=1408, out_features=5632, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=5632, out_features=1408, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Block(\n",
       "            (dwconv): Conv2d(2816, 2816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2816)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (dwconv): Conv2d(2816, 2816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2816)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (dwconv): Conv2d(2816, 2816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2816)\n",
       "            (norm): LayerNorm()\n",
       "            (pwconv1): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "            (act): GELU()\n",
       "            (grn): GRN()\n",
       "            (pwconv2): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((2816,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Linear(in_features=2816, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mkpts_as_q): Transformer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1000, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (cnn_as_q): Transformer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=1000, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (norm1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mlp2): Sequential(\n",
       "    (0): Linear(in_features=4000, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): Dropout(p=0.1, inplace=False)\n",
       "    (15): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (16): LeakyReLU(negative_slope=0.01)\n",
       "    (17): Dropout(p=0.1, inplace=False)\n",
       "    (18): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (19): LeakyReLU(negative_slope=0.01)\n",
       "    (20): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (translation_head): Linear(in_features=32, out_features=3, bias=True)\n",
       "  (rotation_head): Linear(in_features=32, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.load('./weights/20240505/ycbv-imgs-relative_r-gt_t-6d-300-2024-05-04-17-45-22-13410.5029.pth').to(device)\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name_dict = {\n",
    "    1: \"ape\",\n",
    "    2: \"benchvise\",\n",
    "    4: \"camera\",\n",
    "    5: \"can\",\n",
    "    6: \"cat\",\n",
    "    8: \"driller\",\n",
    "    9: \"duck\",\n",
    "    10: \"eggbox\",\n",
    "    11: \"glue\",\n",
    "    12: \"holepuncher\",\n",
    "    13: \"iron\",\n",
    "    14: \"lamp\",\n",
    "    15: \"phone\",\n",
    "}\n",
    "\n",
    "ycbv_dict = {\n",
    "    1: 'one',\n",
    "    2: 'two',\n",
    "    3: 'three',\n",
    "    4: 'four',\n",
    "    5: 'five',\n",
    "    6: 'six',\n",
    "    7: 'seven',\n",
    "    8: 'eight',\n",
    "    9: 'nine',\n",
    "    10: 'ten',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linemod\n",
    "ape_data = []\n",
    "benchvise_data = []\n",
    "camera_data = []\n",
    "can_data = []\n",
    "cat_data = []\n",
    "driller_data = []\n",
    "duck_data = []\n",
    "eggbox_data = []\n",
    "glue_data = []\n",
    "holepuncher_data = []\n",
    "iron_data = []\n",
    "lamp_data = []\n",
    "phone_data = []\n",
    "# onepose\n",
    "aptamil_data = []\n",
    "jzhg_data = []\n",
    "minipuff_data = []\n",
    "hlyormosiapie_data = []\n",
    "brownhouse_data = []\n",
    "oreo_data = []\n",
    "mfmilkcake_data = []\n",
    "diycookies_data = []\n",
    "taipingcookies_data = []\n",
    "tee_data = []\n",
    "# onepose++\n",
    "toyrobot_data = []\n",
    "yellowduck_data = []\n",
    "sheep_data = []\n",
    "fakebanana_data = []\n",
    "teabox_data = []\n",
    "orange_data = []\n",
    "greenteapot_data = []\n",
    "lecreusetcup_data = []\n",
    "insta_data = []\n",
    "# ycbv\n",
    "one_data = []\n",
    "two_data = []\n",
    "three_data = []\n",
    "four_data = []\n",
    "five_data = []\n",
    "six_data = []\n",
    "seven_data = []\n",
    "eight_data = []\n",
    "nine_data = []\n",
    "ten_data = []\n",
    "\n",
    "all_data = {\n",
    "    # linemod\n",
    "    'ape_data': ape_data,\n",
    "    'benchvise_data': benchvise_data,\n",
    "    'camera_data': camera_data,\n",
    "    'can_data': can_data,\n",
    "    'cat_data': cat_data,\n",
    "    'driller_data': driller_data,\n",
    "    'duck_data': duck_data,\n",
    "    'eggbox_data': eggbox_data,\n",
    "    'glue_data': glue_data,\n",
    "    'holepuncher_data': holepuncher_data,\n",
    "    'iron_data': iron_data,\n",
    "    'lamp_data': lamp_data,\n",
    "    'phone_data': phone_data,\n",
    "    # onepose\n",
    "    'aptamil_data': aptamil_data,\n",
    "    'jzhg_data': jzhg_data,\n",
    "    'minipuff_data': minipuff_data,\n",
    "    'hlyormosiapie_data': hlyormosiapie_data,\n",
    "    'brownhouse_data': brownhouse_data,\n",
    "    'oreo_data': oreo_data,\n",
    "    'mfmilkcake_data': mfmilkcake_data,\n",
    "    'diycookies_data': diycookies_data,\n",
    "    'taipingcookies_data': taipingcookies_data,\n",
    "    'tee_data': tee_data,\n",
    "    # onepose++\n",
    "    'toyrobot_data': toyrobot_data,\n",
    "    'yellowduck_data': yellowduck_data,\n",
    "    'sheep_data': sheep_data,\n",
    "    'fakebanana_data': fakebanana_data,\n",
    "    'teabox_data': teabox_data,\n",
    "    'orange_data': orange_data,\n",
    "    'greenteapot_data': greenteapot_data,\n",
    "    'lecreusetcup_data': lecreusetcup_data,\n",
    "    'insta_data': insta_data,\n",
    "    # ycbv\n",
    "    'one_data': one_data,\n",
    "    'two_data': two_data,\n",
    "    'three_data': three_data,\n",
    "    'four_data': four_data,\n",
    "    'five_data': five_data,\n",
    "    'six_data': six_data,\n",
    "    'seven_data': seven_data,\n",
    "    'eight_data': eight_data,\n",
    "    'nine_data': nine_data,\n",
    "    'ten_data': ten_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linemod_type = ['ape_data', 'benchvise_data', 'camera_data', 'can_data', 'cat_data', 'driller_data', 'duck_data', 'eggbox_data', 'glue_data', 'holepuncher_data', 'iron_data', 'lamp_data', 'phone_data']\n",
    "onepose_type = ['aptamil_data', 'jzhg_data', 'minipuff_data', 'hlyormosiapie_data', 'brownhouse_data', 'oreo_data', 'mfmilkcake_data', 'diycookies_data', 'taipingcookies_data', 'tee_data']\n",
    "oneposeplusplus_type = ['toyrobot_data', 'yellowduck_data', 'sheep_data', 'fakebanana_data', 'teabox_data', 'orange_data', 'greenteapot_data', 'lecreusetcup_data', 'insta_data']\n",
    "ycbv_type = ['one_data', 'two_data', 'three_data', 'four_data', 'five_data', 'six_data', 'seven_data', 'eight_data', 'nine_data', 'ten_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_data 41\n",
      "two_data 28\n",
      "three_data 54\n",
      "four_data 31\n",
      "five_data 28\n",
      "six_data 31\n",
      "seven_data 4\n",
      "eight_data 17\n",
      "nine_data 21\n",
      "ten_data 25\n",
      "len(all_data): 10\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_dataloader):\n",
    "    for data in batch:\n",
    "        if 'lm' in data['name']:\n",
    "            all_data[f\"{id2name_dict[int(data['name'][2:])]}_data\"].append(data)\n",
    "        else:\n",
    "            if data['name'] in ('12345678910'):\n",
    "                all_data[f\"{ycbv_dict[int(data['name'])]}_data\"].append(data)\n",
    "            else:\n",
    "                all_data[f\"{data['name']}_data\"].append(data)\n",
    "\n",
    "empty_keys = []\n",
    "for key in all_data.keys():\n",
    "    if len(all_data[key]) == 0:\n",
    "        empty_keys.append(key)\n",
    "\n",
    "for key in empty_keys:\n",
    "    all_data.pop(key)\n",
    "\n",
    "for key in all_data.keys():\n",
    "    print(key, len(all_data[key]))\n",
    "\n",
    "print('len(all_data):', len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:44.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: one_data\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:45.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 41 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:45.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: two_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 18/41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:46.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 28 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:46.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: three_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 15/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:47.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 54 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:47.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: four_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 13/54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:48.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 31 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:48.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: five_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 8/31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:49.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 28 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:49.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: six_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 18/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:50.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 31 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:50.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: seven_data\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:50.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 4 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:50.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: eight_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 9/31\n",
      "Acc: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:50.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 17 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:50.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: nine_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 6/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:51.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 21 unique items...\u001b[0m\n",
      "\u001b[32m2024-07-13 18:44:51.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mYCBV: ten_data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 10/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-13 18:44:51.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpose.utils\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mAggregating metrics over 25 unique items...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 18/25\n"
     ]
    }
   ],
   "source": [
    "res_table = []\n",
    "\n",
    "model_type = 'relative_r-gt_t'\n",
    "\n",
    "for key in all_data.keys():\n",
    "    if key in linemod_type:\n",
    "        logger.info(f\"LINEMOD: {key}\")\n",
    "    elif key in onepose_type:\n",
    "        logger.info(f\"ONEPOSE: {key}\")\n",
    "    elif key in oneposeplusplus_type:\n",
    "        logger.info(f\"ONEPOSE++: {key}\")\n",
    "    elif key in ycbv_type:\n",
    "        logger.info(f\"YCBV: {key}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    recall_image, all_image = 0, 0\n",
    "    for item in all_data[key]:\n",
    "        all_image += 1\n",
    "        K0 = item['K0']\n",
    "        K1 = item['K1']\n",
    "        pose0 = item['pose0']\n",
    "        pose1 = item['pose1']\n",
    "        pre_bbox = item['pre_bbox']\n",
    "        gt_bbox = item['gt_bbox']\n",
    "        mkpts0 = item['mkpts0']\n",
    "        mkpts1 = item['mkpts1']\n",
    "        pre_K = item['pre_K']\n",
    "        img0 = item['img0']\n",
    "        img1 = item['img1']\n",
    "        name = item['name']\n",
    "        pair_name = item['pair_name']\n",
    "        # linemod\n",
    "        if 'lm' in name:\n",
    "            name = id2name_dict[int(name[2:])]\n",
    "        # ycbv\n",
    "        if name in '12345678910':\n",
    "            name = ycbv_dict[int(name)]\n",
    "\n",
    "        if name not in key:\n",
    "            print(f'name: {name}, key: {key}')\n",
    "            continue\n",
    "\n",
    "        is_recalled = recall_object(pre_bbox, gt_bbox)\n",
    "\n",
    "        recall_image = recall_image + int(is_recalled > 0.5)\n",
    "\n",
    "        batch_mkpts0 = torch.from_numpy(mkpts0).unsqueeze(0).float().to(device)\n",
    "        batch_mkpts1 = torch.from_numpy(mkpts1).unsqueeze(0).float().to(device)\n",
    "        img0 = torch.from_numpy(img0).unsqueeze(0).float().to(device)\n",
    "        img1 = torch.from_numpy(img1).unsqueeze(0).float().to(device)\n",
    "        img0 = img0.permute(0, 3, 2, 1)\n",
    "        img1 = img1.permute(0, 3, 2, 1)\n",
    "        pre_t, pre_rot = net(batch_mkpts0, batch_mkpts1, img0, img1)\n",
    "        pre_t = pre_t.squeeze(0).detach().cpu().numpy()\n",
    "        pre_rot = pre_rot.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        if model_type == 'gt':\n",
    "            t_err, R_err = relative_pose_error_np(pose1, pre_rot, pre_t, ignore_gt_t_thr=0.0)\n",
    "        elif model_type == 'relative':\n",
    "            relative_pose = np.matmul(pose1, np.linalg.inv(pose0))\n",
    "            t_err, R_err = relative_pose_error_np(relative_pose, pre_rot, pre_t, ignore_gt_t_thr=0.0)\n",
    "        elif model_type == 'relative_r-gt_t':\n",
    "            relative_pose = np.matmul(pose1, np.linalg.inv(pose0))\n",
    "            gt_pose = np.zeros_like(pose1)\n",
    "            gt_pose[:3, :3] = relative_pose[:3, :3]\n",
    "            gt_pose[:3, 3] = pose1[:3, 3]\n",
    "            t_err, R_err = relative_pose_error_np(gt_pose, pre_rot, pre_t, ignore_gt_t_thr=0.0)\n",
    "\n",
    "        metrics['R_errs'].append(R_err)\n",
    "        metrics['t_errs'].append(t_err)\n",
    "        metrics['identifiers'].append(pair_name)\n",
    "\n",
    "    print(f\"Acc: {recall_image}/{all_image}\")\n",
    "    val_metrics_4tb = aggregate_metrics(metrics, 5e-4)\n",
    "    val_metrics_4tb[\"AP50\"] = recall_image / all_image\n",
    "    # logger.info('\\n' + pprint.pformat(val_metrics_4tb))\n",
    "    res_table.append([f\"{name}\"] + list(val_metrics_4tb.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 18:44:51\n",
      "YCBV\n",
      "╒════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════════╤═════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════════╤═════════════╤══════════╕\n",
      "│ Category   │   R:auc@ 1 │   R:auc@ 2 │   R:auc@ 3 │   R:auc@ 4 │   R:auc@ 5 │   R:auc@ 6 │   R:auc@ 7 │   R:auc@ 8 │   R:auc@ 9 │   R:auc@10 │   R:auc@11 │   R:auc@12 │   R:auc@13 │   R:auc@14 │   R:auc@15 │   R:auc@16 │   R:auc@17 │   R:auc@18 │   R:auc@19 │   R:auc@20 │   R:auc@21 │   R:auc@22 │   R:auc@23 │   R:auc@24 │   R:auc@25 │   R:auc@26 │   R:auc@27 │   R:auc@28 │   R:auc@29 │   R:auc@30 │   R:ACC 1 │   R:ACC 2 │   R:ACC 3 │   R:ACC 4 │   R:ACC 5 │   R:ACC 6 │   R:ACC 7 │   R:ACC 8 │   R:ACC 9 │   R:ACC10 │   R:ACC11 │   R:ACC12 │   R:ACC13 │   R:ACC14 │   R:ACC15 │   R:ACC16 │   R:ACC17 │   R:ACC18 │   R:ACC19 │   R:ACC20 │   R:ACC21 │   R:ACC22 │   R:ACC23 │   R:ACC24 │   R:ACC25 │   R:ACC26 │   R:ACC27 │   R:ACC28 │   R:ACC29 │   R:ACC30 │   R:medianErr │   R:meanErr │   t:auc@ 1 │   t:auc@ 2 │   t:auc@ 3 │   t:auc@ 4 │   t:auc@ 5 │   t:auc@ 6 │   t:auc@ 7 │   t:auc@ 8 │   t:auc@ 9 │   t:auc@10 │   t:auc@11 │   t:auc@12 │   t:auc@13 │   t:auc@14 │   t:auc@15 │   t:auc@16 │   t:auc@17 │   t:auc@18 │   t:auc@19 │   t:auc@20 │   t:auc@21 │   t:auc@22 │   t:auc@23 │   t:auc@24 │   t:auc@25 │   t:auc@26 │   t:auc@27 │   t:auc@28 │   t:auc@29 │   t:auc@30 │   t:ACC 1 │   t:ACC 2 │   t:ACC 3 │   t:ACC 4 │   t:ACC 5 │   t:ACC 6 │   t:ACC 7 │   t:ACC 8 │   t:ACC 9 │   t:ACC10 │   t:ACC11 │   t:ACC12 │   t:ACC13 │   t:ACC14 │   t:ACC15 │   t:ACC16 │   t:ACC17 │   t:ACC18 │   t:ACC19 │   t:ACC20 │   t:ACC21 │   t:ACC22 │   t:ACC23 │   t:ACC24 │   t:ACC25 │   t:ACC26 │   t:ACC27 │   t:ACC28 │   t:ACC29 │   t:ACC30 │   t:medianErr │   t:meanErr │     AP50 │\n",
      "╞════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════════╪═════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════════╪═════════════╪══════════╡\n",
      "│ one        │  0.0381226 │  0.0808794 │  0.122358  │  0.152294  │  0.174097  │  0.189796  │  0.206292  │  0.222672  │  0.235871  │   0.250216 │  0.266026  │  0.280443  │   0.296097 │   0.311184 │   0.324585 │   0.33631  │   0.350727 │   0.363762 │   0.377043 │   0.388679 │   0.40168  │   0.415211 │   0.42915  │   0.442773 │   0.456385 │   0.471285 │   0.487534 │   0.503223 │   0.519637 │   0.53565  │ 0.0731707 │ 0.146341  │ 0.219512  │ 0.243902  │ 0.268293  │ 0.268293  │ 0.292683  │  0.341463 │  0.341463 │  0.365854 │  0.439024 │  0.439024 │  0.487805 │  0.512195 │  0.512195 │  0.512195 │  0.585366 │  0.585366 │  0.609756 │  0.609756 │  0.658537 │  0.707317 │  0.731707 │  0.756098 │  0.804878 │  0.853659 │  0.926829 │  0.926829 │  0.97561  │         1 │      13.7183  │    14.2927  │          0 │  0         │  0         │  0.0434097 │  0.120975  │  0.204189  │  0.286802  │  0.355382  │  0.415057  │   0.468082 │   0.515615 │   0.555981 │   0.590136 │   0.619412 │   0.644785 │   0.666986 │   0.686575 │   0.703987 │   0.719567 │   0.733588 │   0.746275 │   0.757808 │   0.768338 │   0.77799  │   0.786871 │   0.795068 │   0.802658 │   0.809706 │   0.816268 │   0.822392 │         0 │  0        │ 0         │ 0.317073  │ 0.487805  │  0.731707 │ 0.804878  │  0.853659 │  0.902439 │  0.95122  │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       5.08452 │     5.45531 │ 0.439024 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ two        │  0.0339184 │  0.0695361 │  0.115201  │  0.158734  │  0.191273  │  0.212965  │  0.238141  │  0.253016  │  0.264586  │   0.282854 │  0.298322  │  0.315361  │   0.329564 │   0.346183 │   0.360876 │   0.374035 │   0.392341 │   0.408243 │   0.42247  │   0.43884  │   0.453583 │   0.46868  │   0.484454 │   0.501057 │   0.518407 │   0.536658 │   0.553819 │   0.569754 │   0.58459  │   0.598437 │ 0.0714286 │ 0.142857  │ 0.25      │ 0.321429  │ 0.321429  │ 0.321429  │ 0.357143  │  0.357143 │  0.357143 │  0.428571 │  0.464286 │  0.5      │  0.5      │  0.535714 │  0.571429 │  0.571429 │  0.678571 │  0.678571 │  0.678571 │  0.714286 │  0.785714 │  0.785714 │  0.821429 │  0.892857 │  0.928571 │  1        │  1        │  1        │  1        │         1 │      12.2574  │    12.5048  │          0 │  0         │  0         │  0         │  0         │  0         │  0.018344  │  0.0316301 │  0.0738534 │   0.1524   │   0.229338 │   0.29356  │   0.347902 │   0.39448  │   0.434848 │   0.47017  │   0.501336 │   0.52904  │   0.553827 │   0.576136 │   0.59632  │   0.614669 │   0.631423 │   0.64678  │   0.660909 │   0.673951 │   0.686027 │   0.69724  │   0.70768  │   0.717424 │         0 │  0        │ 0         │ 0         │ 0         │  0        │ 0.0357143 │  0.214286 │  0.642857 │  0.964286 │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       8.71006 │     8.66023 │ 0.535714 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ three      │  0         │  0         │  0.019026  │  0.0281584 │  0.0336378 │  0.0372908 │  0.0399    │  0.0492263 │  0.0596192 │   0.07089  │  0.0830955 │  0.0977523 │   0.115391 │   0.132922 │   0.149941 │   0.164875 │   0.180157 │   0.195744 │   0.210783 │   0.226873 │   0.243438 │   0.260684 │   0.278177 │   0.29513  │   0.312338 │   0.329413 │   0.346847 │   0.364168 │   0.382681 │   0.402773 │ 0         │ 0         │ 0.0555556 │ 0.0555556 │ 0.0555556 │ 0.0555556 │ 0.0555556 │  0.111111 │  0.148148 │  0.166667 │  0.222222 │  0.314815 │  0.333333 │  0.37037  │  0.388889 │  0.388889 │  0.425926 │  0.481481 │  0.481481 │  0.537037 │  0.592593 │  0.648148 │  0.666667 │  0.703704 │  0.740741 │  0.759259 │  0.814815 │  0.851852 │  0.925926 │         1 │      19.2361  │    18.1905  │          0 │  0         │  0.0207724 │  0.0470735 │  0.0672885 │  0.101457  │  0.169319  │  0.245121  │  0.316852  │   0.382291 │   0.436763 │   0.484654 │   0.524296 │   0.558275 │   0.587723 │   0.613491 │   0.636226 │   0.656436 │   0.674518 │   0.690792 │   0.705517 │   0.718902 │   0.731124 │   0.742327 │   0.752634 │   0.762148 │   0.770957 │   0.779137 │   0.786753 │   0.793862 │         0 │  0        │ 0.0925926 │ 0.148148  │ 0.148148  │  0.407407 │ 0.685185  │  0.814815 │  0.925926 │  0.981481 │  0.981481 │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       6.17839 │     6.28959 │ 0.240741 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ four       │  0         │  0         │  0.0219871 │  0.0326194 │  0.0530109 │  0.0656811 │  0.0828756 │  0.100931  │  0.122262  │   0.142896 │  0.167083  │  0.190731  │   0.214954 │   0.238222 │   0.267898 │   0.301211 │   0.330931 │   0.360277 │   0.38749  │   0.411664 │   0.433536 │   0.456269 │   0.475702 │   0.495104 │   0.512719 │   0.528979 │   0.546451 │   0.561497 │   0.577358 │   0.591446 │ 0         │ 0         │ 0.0645161 │ 0.0645161 │ 0.129032  │ 0.129032  │ 0.193548  │  0.225806 │  0.290323 │  0.354839 │  0.419355 │  0.483871 │  0.516129 │  0.548387 │  0.774194 │  0.806452 │  0.806452 │  0.83871  │  0.870968 │  0.870968 │  0.870968 │  0.903226 │  0.903226 │  0.935484 │  0.935484 │  0.935484 │  0.967742 │  0.967742 │  1        │         1 │      12.8444  │    12.7168  │          0 │  0         │  0         │  0.0161709 │  0.0260511 │  0.0502434 │  0.0861955 │  0.132205  │  0.178769  │   0.227852 │   0.278246 │   0.330923 │   0.379959 │   0.421943 │   0.45833  │   0.490168 │   0.52371  │   0.550171 │   0.573846 │   0.595154 │   0.614432 │   0.631958 │   0.64796  │   0.662628 │   0.676123 │   0.68858  │   0.700114 │   0.710824 │   0.720796 │   0.730103 │         0 │  0        │ 0         │ 0.0322581 │ 0.0967742 │  0.193548 │ 0.387097  │  0.516129 │  0.548387 │  0.741935 │  0.806452 │  0.935484 │  0.967742 │  0.967742 │  0.967742 │  0.967742 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       7.63905 │     8.35597 │ 0.258065 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ five       │  0.0252442 │  0.0498476 │  0.0859931 │  0.124753  │  0.149802  │  0.181663  │  0.212967  │  0.247177  │  0.275269  │   0.297742 │  0.316129  │  0.340636  │   0.35839  │   0.373607 │   0.386795 │   0.398334 │   0.418003 │   0.432479 │   0.445432 │   0.460294 │   0.474103 │   0.48884  │   0.501747 │   0.513579 │   0.527288 │   0.540881 │   0.55674  │   0.572571 │   0.58731  │   0.601066 │ 0.0357143 │ 0.0714286 │ 0.178571  │ 0.25      │ 0.25      │ 0.357143  │ 0.464286  │  0.5      │  0.5      │  0.5      │  0.5      │  0.571429 │  0.571429 │  0.571429 │  0.571429 │  0.571429 │  0.678571 │  0.678571 │  0.678571 │  0.714286 │  0.75     │  0.785714 │  0.785714 │  0.785714 │  0.821429 │  0.892857 │  1        │  1        │  1        │         1 │       9.55959 │    12.4456  │          0 │  0.0523867 │  0.0866823 │  0.130318  │  0.191752  │  0.237174  │  0.269619  │  0.293952  │  0.312878  │   0.337754 │   0.373136 │   0.425355 │   0.469558 │   0.507447 │   0.540284 │   0.569016 │   0.594368 │   0.616903 │   0.637066 │   0.655213 │   0.671631 │   0.686557 │   0.700185 │   0.712677 │   0.72417  │   0.734779 │   0.744602 │   0.753724 │   0.762216 │   0.770142 │         0 │  0.107143 │ 0.214286  │ 0.392857  │ 0.464286  │  0.464286 │ 0.464286  │  0.464286 │  0.464286 │  0.5      │  0.964286 │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       9.86018 │     7.09273 │ 0.642857 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ six        │  0.0358232 │  0.0662987 │  0.0991707 │  0.114701  │  0.135247  │  0.157596  │  0.171949  │  0.182713  │  0.198277  │   0.207481 │  0.218144  │  0.229285  │   0.243527 │   0.25778  │   0.273232 │   0.290336 │   0.307664 │   0.326073 │   0.342867 │   0.357982 │   0.371657 │   0.384089 │   0.401036 │   0.416724 │   0.431023 │   0.446687 │   0.462138 │   0.478683 │   0.495547 │   0.512754 │ 0.0967742 │ 0.0967742 │ 0.16129   │ 0.16129   │ 0.193548  │ 0.258065  │ 0.258065  │  0.258065 │  0.290323 │  0.290323 │  0.322581 │  0.354839 │  0.419355 │  0.451613 │  0.516129 │  0.548387 │  0.580645 │  0.645161 │  0.645161 │  0.645161 │  0.645161 │  0.645161 │  0.709677 │  0.774194 │  0.774194 │  0.83871  │  0.870968 │  0.967742 │  0.967742 │         1 │      14.7896  │    15.0905  │          0 │  0         │  0.0525869 │  0.0897055 │  0.140083  │  0.197314  │  0.25403   │  0.306003  │  0.357109  │   0.407621 │   0.459798 │   0.504815 │   0.542906 │   0.575556 │   0.603852 │   0.628611 │   0.650458 │   0.669877 │   0.687252 │   0.702889 │   0.717037 │   0.729899 │   0.741643 │   0.752408 │   0.762311 │   0.771453 │   0.779918 │   0.787778 │   0.795096 │   0.801926 │         0 │  0        │ 0.16129   │ 0.225806  │ 0.387097  │  0.548387 │ 0.612903  │  0.709677 │  0.774194 │  0.935484 │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       5.84936 │     6.11539 │ 0.290323 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ seven      │  0.193927  │  0.351904  │  0.501836  │  0.636076  │  0.708861  │  0.757384  │  0.792043  │  0.818038  │  0.838256  │   0.85443  │  0.867664  │  0.878692  │   0.888023 │   0.896022 │   0.902954 │   0.909019 │   0.914371 │   0.919128 │   0.923384 │   0.927215 │   0.930681 │   0.933832 │   0.936709 │   0.939346 │   0.941772 │   0.944012 │   0.946085 │   0.948011 │   0.949804 │   0.951477 │ 0.25      │ 0.5       │ 0.75      │ 1         │ 1         │ 1         │ 1         │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │         1 │       1.7932  │     1.90265 │          0 │  0         │  0         │  0.135981  │  0.214515  │  0.34955   │  0.442471  │  0.512162  │  0.566366  │   0.60973  │   0.645209 │   0.674775 │   0.699792 │   0.721236 │   0.73982  │   0.756081 │   0.770429 │   0.783183 │   0.794595 │   0.804865 │   0.814157 │   0.822604 │   0.830317 │   0.837387 │   0.843892 │   0.849896 │   0.855455 │   0.860618 │   0.865424 │   0.86991  │         0 │  0        │ 0         │ 0.25      │ 0.75      │  1        │ 1         │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       4.7075  │     4.5395  │ 0.75     │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ eight      │  0.0399462 │  0.0792826 │  0.0920708 │  0.122548  │  0.144509  │  0.15964   │  0.170448  │  0.178553  │  0.184858  │   0.189902 │  0.214384  │  0.22593   │   0.2357   │   0.252183 │   0.262821 │   0.27213  │   0.288347 │   0.303147 │   0.315056 │   0.325774 │   0.335471 │   0.344286 │   0.364051 │   0.378294 │   0.394616 │   0.408851 │   0.42203  │   0.438584 │   0.455847 │   0.474412 │ 0.0588235 │ 0.117647  │ 0.117647  │ 0.176471  │ 0.235294  │ 0.235294  │ 0.235294  │  0.235294 │  0.235294 │  0.235294 │  0.352941 │  0.352941 │  0.352941 │  0.411765 │  0.411765 │  0.411765 │  0.470588 │  0.529412 │  0.529412 │  0.529412 │  0.529412 │  0.529412 │  0.705882 │  0.705882 │  0.764706 │  0.764706 │  0.764706 │  0.882353 │  0.941176 │         1 │      17.0942  │    16.6297  │          0 │  0         │  0         │  0         │  0.0373633 │  0.106754  │  0.184726  │  0.278594  │  0.35875   │   0.422875 │   0.475341 │   0.519063 │   0.556058 │   0.587768 │   0.61525  │   0.639297 │   0.660515 │   0.679375 │   0.69625  │   0.711438 │   0.725179 │   0.737671 │   0.749076 │   0.759531 │   0.76915  │   0.778029 │   0.78625  │   0.793884 │   0.800991 │   0.807625 │         0 │  0        │ 0         │ 0         │ 0.176471  │  0.529412 │ 0.705882  │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       5.9564  │     5.99798 │ 0.352941 │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ nine       │  0.023997  │  0.062053  │  0.0889877 │  0.102455  │  0.141632  │  0.172762  │  0.206927  │  0.240305  │  0.271178  │   0.296441 │  0.322519  │  0.343262  │   0.366807 │   0.384824 │   0.400439 │   0.414102 │   0.426158 │   0.444679 │   0.456362 │   0.466878 │   0.480858 │   0.493603 │   0.505268 │   0.515961 │   0.529005 │   0.542386 │   0.555807 │   0.570561 │   0.585153 │   0.598981 │ 0.047619  │ 0.142857  │ 0.142857  │ 0.142857  │ 0.285714  │ 0.333333  │ 0.428571  │  0.47619  │  0.52381  │  0.52381  │  0.571429 │  0.571429 │  0.619048 │  0.619048 │  0.619048 │  0.619048 │  0.619048 │  0.666667 │  0.666667 │  0.666667 │  0.714286 │  0.761905 │  0.761905 │  0.761905 │  0.809524 │  0.904762 │  0.904762 │  0.952381 │  1        │         1 │       8.9098  │    12.7107  │          0 │  0.0615546 │  0.142186  │  0.220471  │  0.281409  │  0.321809  │  0.350666  │  0.385242  │  0.412222  │   0.446001 │   0.49112  │   0.529558 │   0.562083 │   0.589962 │   0.622916 │   0.646484 │   0.667279 │   0.685763 │   0.702302 │   0.717187 │   0.730654 │   0.742897 │   0.754076 │   0.764323 │   0.77375  │   0.782452 │   0.790509 │   0.797991 │   0.804957 │   0.811458 │         0 │  0.142857 │ 0.380952  │ 0.47619   │ 0.52381   │  0.52381  │ 0.52381   │  0.571429 │  0.619048 │  0.904762 │  0.952381 │  0.952381 │  0.952381 │  0.952381 │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       4.04922 │     5.99014 │ 0.47619  │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ ten        │  0.0464458 │  0.0832229 │  0.126384  │  0.164788  │  0.21126   │  0.256269  │  0.28823   │  0.312202  │  0.34111   │   0.363499 │  0.387545  │  0.408583  │   0.431507 │   0.449257 │   0.468626 │   0.484337 │   0.498199 │   0.514037 │   0.526983 │   0.541075 │   0.553405 │   0.564614 │   0.577914 │   0.588834 │   0.598881 │   0.608154 │   0.621094 │   0.633188 │   0.645761 │   0.657569 │ 0.12      │ 0.12      │ 0.28      │ 0.28      │ 0.44      │ 0.48      │ 0.48      │  0.48     │  0.52     │  0.56     │  0.64     │  0.64     │  0.68     │  0.68     │  0.72     │  0.72     │  0.72     │  0.76     │  0.76     │  0.8      │  0.8      │  0.8      │  0.84     │  0.84     │  0.84     │  0.84     │  0.92     │  0.96     │  1        │         1 │       8.28256 │    10.8493  │          0 │  0         │  0.0357131 │  0.0957961 │  0.178017  │  0.283049  │  0.374246  │  0.452466  │  0.513303  │   0.561972 │   0.601793 │   0.634977 │   0.663056 │   0.687123 │   0.707982 │   0.726233 │   0.742337 │   0.756651 │   0.769459 │   0.780986 │   0.791415 │   0.800897 │   0.809553 │   0.817489 │   0.824789 │   0.831528 │   0.837768 │   0.843562 │   0.848956 │   0.853991 │         0 │  0        │ 0.12      │ 0.36      │ 0.68      │  0.84     │ 1         │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │  1        │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       4.55282 │     4.51414 │ 0.72     │\n",
      "├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────────┼─────────────┼──────────┤\n",
      "│ Avg        │  0.0437425 │  0.0843025 │  0.127301  │  0.163713  │  0.194333  │  0.219105  │  0.240977  │  0.260484  │  0.279129  │   0.295635 │  0.314091  │  0.331068  │   0.347996 │   0.364218 │   0.379817 │   0.394469 │   0.41069  │   0.426757 │   0.440787 │   0.454527 │   0.467841 │   0.481011 │   0.495421 │   0.50868  │   0.522243 │   0.53573  │   0.549855 │   0.564024 │   0.578369 │   0.592457 │ 0.075353  │ 0.133791  │ 0.221995  │ 0.269602  │ 0.317887  │ 0.343814  │ 0.376515  │  0.398507 │  0.42065  │  0.442536 │  0.493184 │  0.522835 │  0.548004 │  0.570052 │  0.608508 │  0.614959 │  0.656517 │  0.686394 │  0.692059 │  0.708757 │  0.734667 │  0.75666  │  0.792621 │  0.815584 │  0.841953 │  0.878944 │  0.916982 │  0.95089  │  0.981045 │         1 │      11.8485  │    12.7333  │          0 │  0.0113941 │  0.0337941 │  0.0778925 │  0.125745  │  0.185154  │  0.243642  │  0.299276  │  0.350516  │   0.401658 │   0.450636 │   0.495366 │   0.533575 │   0.56632  │   0.595579 │   0.620654 │   0.643323 │   0.663139 │   0.680868 │   0.696825 │   0.711262 │   0.724386 │   0.736369 │   0.747354 │   0.75746  │   0.766788 │   0.775426 │   0.783446 │   0.790914 │   0.797883 │         0 │  0.025    │ 0.0969121 │ 0.220233  │ 0.371439  │  0.523856 │ 0.621976  │  0.714428 │  0.787714 │  0.897917 │  0.97046  │  0.988787 │  0.992012 │  0.992012 │  0.996774 │  0.996774 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │         1 │       6.25875 │     6.3011  │ 0.470586 │\n",
      "╘════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════════╧═════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════════╧═════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "for i, key in enumerate(all_data.keys()):\n",
    "    if i == 1:\n",
    "        break\n",
    "    if key in linemod_type:\n",
    "        print(f\"LINEMOD\")\n",
    "    elif key in onepose_type:\n",
    "        print(f\"ONEPOSE\")\n",
    "    elif key in oneposeplusplus_type:\n",
    "        print(f\"ONEPOSE++\")\n",
    "    elif key in ycbv_type:\n",
    "        print(f\"YCBV\")\n",
    "\n",
    "from tabulate import tabulate\n",
    "headers = [\"Category\"] + list(val_metrics_4tb.keys())\n",
    "all_data = np.array(res_table)[:, 1:].astype(np.float32)\n",
    "res_table.append([\"Avg\"] + all_data.mean(0).tolist())\n",
    "print(tabulate(res_table, headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(res_table, columns=headers)\n",
    "\n",
    "df_rounded = df.round(6)\n",
    "\n",
    "df_rounded.to_excel(\"res.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
