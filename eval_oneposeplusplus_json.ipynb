{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_name: ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross']\n",
      "layer_name: ['self', 'cross']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-21 10:23:07.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpope_model_api\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mload Matcher successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pope_model_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-21 10:23:18.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mload SAM model from weights/sam_vit_h_4b8939.pth\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): DropPath()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): DropPath()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt, model_type = get_model_info(\"h\")\n",
    "sam = sam_model_registry[model_type](checkpoint=ckpt)\n",
    "DEVICE = \"cuda\"\n",
    "sam.to(device=DEVICE)\n",
    "MASK_GEN = SamAutomaticMaskGenerator(sam)\n",
    "logger.info(f\"load SAM model from {ckpt}\")\n",
    "crop_tool = CropImage()\n",
    "dinov2_model = load_dinov2_model()\n",
    "dinov2_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name_dict = {\n",
    "    1: \"ape\",\n",
    "    2: \"benchvise\",\n",
    "    4: \"camera\",\n",
    "    5: \"can\",\n",
    "    6: \"cat\",\n",
    "    8: \"driller\",\n",
    "    9: \"duck\",\n",
    "    10: \"eggbox\",\n",
    "    11: \"glue\",\n",
    "    12: \"holepuncher\",\n",
    "    13: \"iron\",\n",
    "    14: \"lamp\",\n",
    "    15: \"phone\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "ROOT_DIR = \"data/oneposeplusplus/\"\n",
    "res_table = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/pairs/OneposePlusPlus-test.json\") as f:\n",
    "    dir_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onepose: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:42<00:00,  3.94s/it]\n",
      "100%|██████████| 32/32 [02:05<00:00,  3.93s/it]\n",
      "100%|██████████| 34/34 [02:19<00:00,  4.11s/it]\n",
      "100%|██████████| 52/52 [03:49<00:00,  4.40s/it]\n",
      "100%|██████████| 48/48 [03:15<00:00,  4.07s/it]\n",
      "100%|██████████| 47/47 [03:02<00:00,  3.88s/it]\n",
      "\u001b[32m2023-12-21 10:39:34.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 238 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 10:39:34.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.9623430962343096,\n",
      " 'R:ACC15': 0.8075313807531381,\n",
      " 'R:ACC30': 0.9288702928870293,\n",
      " 'R:auc@15': 0.5886456993866334,\n",
      " 'R:auc@30': 0.7337297216749039,\n",
      " 'R:medianErr': 4.202318780464796,\n",
      " 't:ACC15': 0.7364016736401674,\n",
      " 't:ACC30': 0.8326359832635983,\n",
      " 't:auc@15': 0.5140949365497659,\n",
      " 't:auc@30': 0.6578577478988182,\n",
      " 't:medianErr': 5.509344354127372}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 230/239\n",
      "Onepose: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [03:03<00:00,  4.09s/it]\n",
      "100%|██████████| 50/50 [03:20<00:00,  4.00s/it]\n",
      "100%|██████████| 73/73 [04:49<00:00,  3.96s/it]\n",
      "100%|██████████| 55/55 [03:37<00:00,  3.95s/it]\n",
      "100%|██████████| 50/50 [03:16<00:00,  3.92s/it]\n",
      "100%|██████████| 42/42 [02:45<00:00,  3.94s/it]\n",
      "\u001b[32m2023-12-21 11:00:27.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 315 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 11:00:27.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.9809523809523809,\n",
      " 'R:ACC15': 0.765079365079365,\n",
      " 'R:ACC30': 0.9365079365079365,\n",
      " 'R:auc@15': 0.5204829654869927,\n",
      " 'R:auc@30': 0.69583582937741,\n",
      " 'R:medianErr': 5.847260783199984,\n",
      " 't:ACC15': 0.6285714285714286,\n",
      " 't:ACC30': 0.8,\n",
      " 't:auc@15': 0.4027138696796177,\n",
      " 't:auc@30': 0.5670140876697438,\n",
      " 't:medianErr': 9.325372058760749}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 309/315\n",
      "Onepose: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [02:31<00:00,  3.98s/it]\n",
      "100%|██████████| 44/44 [02:56<00:00,  4.01s/it]\n",
      "100%|██████████| 43/43 [02:52<00:00,  4.01s/it]\n",
      "100%|██████████| 35/35 [02:20<00:00,  4.00s/it]\n",
      "100%|██████████| 35/35 [02:19<00:00,  3.99s/it]\n",
      "100%|██████████| 38/38 [02:30<00:00,  3.97s/it]\n",
      "\u001b[32m2023-12-21 11:15:58.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 233 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 11:15:58.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.3562231759656652,\n",
      " 'R:ACC15': 0.7982832618025751,\n",
      " 'R:ACC30': 0.9141630901287554,\n",
      " 'R:auc@15': 0.5596144044770406,\n",
      " 'R:auc@30': 0.7054162755133803,\n",
      " 'R:medianErr': 4.946690999921155,\n",
      " 't:ACC15': 0.5879828326180258,\n",
      " 't:ACC30': 0.7339055793991416,\n",
      " 't:auc@15': 0.3735198036466297,\n",
      " 't:auc@30': 0.525093651537986,\n",
      " 't:medianErr': 10.493588995448029}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 83/233\n",
      "Onepose: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [02:45<00:00,  3.93s/it]\n",
      "100%|██████████| 62/62 [04:10<00:00,  4.03s/it]\n",
      "100%|██████████| 58/58 [03:53<00:00,  4.03s/it]\n",
      "100%|██████████| 45/45 [03:01<00:00,  4.03s/it]\n",
      "100%|██████████| 44/44 [02:54<00:00,  3.96s/it]\n",
      "100%|██████████| 49/49 [03:13<00:00,  3.94s/it]\n",
      "\u001b[32m2023-12-21 11:35:56.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 300 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 11:35:56.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.97,\n",
      " 'R:ACC15': 0.72,\n",
      " 'R:ACC30': 0.89,\n",
      " 'R:auc@15': 0.5053514544206705,\n",
      " 'R:auc@30': 0.6563686612989826,\n",
      " 'R:medianErr': 5.660132269230592,\n",
      " 't:ACC15': 0.5633333333333334,\n",
      " 't:ACC30': 0.67,\n",
      " 't:auc@15': 0.3421009708653964,\n",
      " 't:auc@30': 0.48435214615249916,\n",
      " 't:medianErr': 11.667475896135244}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 291/300\n",
      "Onepose: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [04:12<00:00,  4.01s/it]\n",
      "100%|██████████| 66/66 [04:28<00:00,  4.06s/it]\n",
      "100%|██████████| 58/58 [03:54<00:00,  4.04s/it]\n",
      "100%|██████████| 57/57 [03:48<00:00,  4.02s/it]\n",
      "100%|██████████| 67/67 [04:36<00:00,  4.13s/it]\n",
      "100%|██████████| 61/61 [04:02<00:00,  3.98s/it]\n",
      "\u001b[32m2023-12-21 12:00:59.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 371 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 12:00:59.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.978494623655914,\n",
      " 'R:ACC15': 0.8521505376344086,\n",
      " 'R:ACC30': 0.9139784946236559,\n",
      " 'R:auc@15': 0.6551409649022112,\n",
      " 'R:auc@30': 0.772990090438306,\n",
      " 'R:medianErr': 2.885757126050291,\n",
      " 't:ACC15': 0.7446236559139785,\n",
      " 't:ACC30': 0.8333333333333334,\n",
      " 't:auc@15': 0.5356388802051871,\n",
      " 't:auc@30': 0.6663737322760733,\n",
      " 't:medianErr': 4.9837196725666075}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 364/372\n",
      "Onepose: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:04<00:00,  3.88s/it]\n",
      "100%|██████████| 63/63 [04:09<00:00,  3.96s/it]\n",
      "100%|██████████| 59/59 [03:49<00:00,  3.89s/it]\n",
      "100%|██████████| 53/53 [03:24<00:00,  3.85s/it]\n",
      "100%|██████████| 66/66 [04:13<00:00,  3.84s/it]\n",
      "100%|██████████| 67/67 [04:16<00:00,  3.83s/it]\n",
      "\u001b[32m2023-12-21 12:22:58.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 340 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 12:22:58.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.9823529411764705,\n",
      " 'R:ACC15': 0.7,\n",
      " 'R:ACC30': 0.9147058823529411,\n",
      " 'R:auc@15': 0.44451839600621745,\n",
      " 'R:auc@30': 0.6391852923681866,\n",
      " 'R:medianErr': 7.7544040938714645,\n",
      " 't:ACC15': 0.5647058823529412,\n",
      " 't:ACC30': 0.7029411764705882,\n",
      " 't:auc@15': 0.3511203189529057,\n",
      " 't:auc@30': 0.5014541348858361,\n",
      " 't:medianErr': 11.911765431716127}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 334/340\n",
      "Onepose: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:39<00:00,  3.98s/it]\n",
      "100%|██████████| 41/41 [02:46<00:00,  4.06s/it]\n",
      "100%|██████████| 30/30 [02:01<00:00,  4.05s/it]\n",
      "100%|██████████| 32/32 [02:09<00:00,  4.03s/it]\n",
      "100%|██████████| 32/32 [02:08<00:00,  4.01s/it]\n",
      "100%|██████████| 34/34 [02:15<00:00,  3.99s/it]\n",
      "\u001b[32m2023-12-21 12:35:58.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 194 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 12:35:58.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.9742268041237113,\n",
      " 'R:ACC15': 0.8402061855670103,\n",
      " 'R:ACC30': 0.9484536082474226,\n",
      " 'R:auc@15': 0.6231622771178348,\n",
      " 'R:auc@30': 0.7640245516155294,\n",
      " 'R:medianErr': 3.514753385016534,\n",
      " 't:ACC15': 0.7525773195876289,\n",
      " 't:ACC30': 0.8298969072164949,\n",
      " 't:auc@15': 0.49132238574081594,\n",
      " 't:auc@30': 0.6514035793176506,\n",
      " 't:medianErr': 6.253364598910034}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 189/194\n",
      "Onepose: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:08<00:00,  4.00s/it]\n",
      "100%|██████████| 35/35 [02:21<00:00,  4.04s/it]\n",
      "100%|██████████| 38/38 [02:30<00:00,  3.97s/it]\n",
      "100%|██████████| 59/59 [03:53<00:00,  3.96s/it]\n",
      "100%|██████████| 30/30 [01:58<00:00,  3.94s/it]\n",
      "100%|██████████| 43/43 [02:59<00:00,  4.17s/it]\n",
      "\u001b[32m2023-12-21 12:51:49.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 237 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 12:51:49.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.8312236286919831,\n",
      " 'R:ACC15': 0.7172995780590717,\n",
      " 'R:ACC30': 0.9113924050632911,\n",
      " 'R:auc@15': 0.5051428167502704,\n",
      " 'R:auc@30': 0.6683269757630026,\n",
      " 'R:medianErr': 6.185688972462571,\n",
      " 't:ACC15': 0.5907172995780591,\n",
      " 't:ACC30': 0.7510548523206751,\n",
      " 't:auc@15': 0.3431786341215846,\n",
      " 't:auc@30': 0.5126974707758618,\n",
      " 't:medianErr': 11.913275354000348}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 197/237\n",
      "Onepose: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [02:19<00:00,  3.87s/it]\n",
      "100%|██████████| 74/74 [04:47<00:00,  3.88s/it]\n",
      "100%|██████████| 103/103 [06:33<00:00,  3.82s/it]\n",
      "100%|██████████| 100/100 [06:18<00:00,  3.78s/it]\n",
      "100%|██████████| 96/96 [05:59<00:00,  3.75s/it]\n",
      "100%|██████████| 112/112 [07:01<00:00,  3.76s/it]\n",
      "\u001b[32m2023-12-21 13:24:48.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.metrics\u001b[0m:\u001b[36maggregate_metrics\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mAggregating metrics over 521 unique items...\u001b[0m\n",
      "\u001b[32m2023-12-21 13:24:48.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\n",
      "{'AP50': 0.6276391554702495,\n",
      " 'R:ACC15': 0.4491362763915547,\n",
      " 'R:ACC30': 0.763915547024952,\n",
      " 'R:auc@15': 0.2653166331013857,\n",
      " 'R:auc@30': 0.4447427676168568,\n",
      " 'R:medianErr': 17.36535157446787,\n",
      " 't:ACC15': 0.2571976967370441,\n",
      " 't:ACC30': 0.3704414587332054,\n",
      " 't:auc@15': 0.14421555334420774,\n",
      " 't:auc@30': 0.22919250635620625,\n",
      " 't:medianErr': 50.39162063782851}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 327/521\n"
     ]
    }
   ],
   "source": [
    "for label_idx, test_dict in enumerate(dir_list):\n",
    "    # print(f\"Onepose: {label_idx}\")\n",
    "    logger.info(f\"OneposePlusPlus: {label_idx}\")\n",
    "    metrics = dict()\n",
    "    metrics.update({'R_errs':[], 't_errs':[], 'inliers':[], \"identifiers\":[]})\n",
    "    sample_data = dir_list[label_idx][\"0\"][0]\n",
    "    label = sample_data.split(\"/\")[0]\n",
    "    name = label.split(\"-\")[1]\n",
    "    dir_name = os.path.dirname(sample_data)\n",
    "    FULL_ROOT_DIR = os.path.join(ROOT_DIR, dir_name)\n",
    "    recall_image, all_image = 0, 0\n",
    "    for rotation_key, rotation_list in zip(test_dict.keys(), test_dict.values()):\n",
    "        for pair_idx,pair_name in enumerate(tqdm(rotation_list)):\n",
    "            all_image = all_image + 1\n",
    "            base_name = os.path.basename(pair_name)\n",
    "            idx0_name = base_name.split(\"-\")[0]\n",
    "            idx1_name = base_name.split(\"-\")[1]\n",
    "            image0_name = os.path.join(FULL_ROOT_DIR, idx0_name)\n",
    "            image1_name = os.path.join(FULL_ROOT_DIR.replace(\"color\", \"color\"), idx1_name)\n",
    "            intrinsic_path = image0_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "            K0 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "            intrinsic_path = image1_name.replace(\"color\", \"intrin_ba\").replace(\"png\", \"txt\")\n",
    "            K1 = np.loadtxt(intrinsic_path, delimiter=' ')\n",
    "\n",
    "            image0 = cv2.imread(image0_name)\n",
    "            ref_torch_image = set_torch_image(image0, center_crop=True)\n",
    "            ref_fea = get_cls_token_torch(dinov2_model, ref_torch_image)\n",
    "            image1 = cv2.imread(image1_name)\n",
    "            image_h, image_w, _ = image1.shape\n",
    "            t1 = time.time()\n",
    "            masks = MASK_GEN.generate(image1)\n",
    "            t2 = time.time()\n",
    "            similarity_score, top_images  = np.array([0, 0, 0], np.float32), [[], [], []]\n",
    "            t3 = time.time()\n",
    "            compact_percent = 0.3\n",
    "            for xxx, mask in enumerate(masks):\n",
    "                object_mask = np.expand_dims(mask[\"segmentation\"], -1)\n",
    "                x0, y0, w, h = mask[\"bbox\"]\n",
    "                x1, y1 = x0 + w, y0 + h\n",
    "                x0 -= int(w * compact_percent)\n",
    "                y0 -= int(h * compact_percent)\n",
    "                x1 += int(w * compact_percent)\n",
    "                y1 += int(h * compact_percent)\n",
    "                box = np.array([x0, y0, x1, y1])\n",
    "                resize_shape = np.array([y1 - y0, x1 - x0])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box, K1, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image1, box, resize_shape)\n",
    "                # object_mask, _ = get_image_crop_resize(object_mask, box, resize_shape)\n",
    "                box_new = np.array([0, 0, x1 - x0, y1 - y0])\n",
    "                resize_shape = np.array([512, 512])\n",
    "                K_crop, K_crop_homo = get_K_crop_resize(box_new, K_crop, resize_shape)\n",
    "                image_crop, _ = get_image_crop_resize(image_crop, box_new, resize_shape)\n",
    "                crop_tensor = set_torch_image(image_crop, center_crop=True)\n",
    "                with torch.no_grad():\n",
    "                    fea = get_cls_token_torch(dinov2_model, crop_tensor)\n",
    "                score = F.cosine_similarity(ref_fea, fea, dim=1, eps=1e-8)\n",
    "                if  (score.item() > similarity_score).any():\n",
    "                    mask[\"crop_image\"] = image_crop\n",
    "                    mask[\"K\"] = K_crop\n",
    "                    mask[\"bbox\"] = box\n",
    "                    min_idx = np.argmin(similarity_score)\n",
    "                    similarity_score[min_idx] = score.item()\n",
    "                    top_images[min_idx] = mask.copy()\n",
    "\n",
    "            img0 = cv2.cvtColor(image0, cv2.COLOR_BGR2GRAY)\n",
    "            img0 = torch.from_numpy(img0).float()[None] / 255.\n",
    "            img0 = img0.unsqueeze(0).cuda()\n",
    "\n",
    "            matching_score = [[0] for _ in range(len(top_images))]\n",
    "            for top_idx in range(len(top_images)):\n",
    "                if \"crop_image\" not in top_images[top_idx]:\n",
    "                    continue\n",
    "                img1 = cv2.cvtColor(top_images[top_idx][\"crop_image\"], cv2.COLOR_BGR2GRAY)\n",
    "                img1 = torch.from_numpy(img1).float()[None] / 255.\n",
    "                img1 = img1.unsqueeze(0).cuda()\n",
    "                batch = {'image0':img0, 'image1':img1}\n",
    "                with torch.no_grad():\n",
    "                    matcher(batch)\n",
    "                    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "                    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "                    confidences = batch[\"mconf\"].cpu().numpy()\n",
    "                conf_mask = np.where(confidences > 0.9)\n",
    "                matching_score[top_idx] = conf_mask[0].shape[0]\n",
    "                top_images[top_idx][\"mkpts0\"] = mkpts0\n",
    "                top_images[top_idx][\"mkpts1\"] = mkpts1\n",
    "                top_images[top_idx][\"mconf\"] = confidences\n",
    "            #---------------------------------------------------\n",
    "            # crop_image = cv2.resize(top_images[np.argmax(matching_score)][\"crop_image\"], (256, 256))\n",
    "            # que_image = cv2.resize(image0, (256, 256))\n",
    "            # image = np.hstack((que_image, crop_image))\n",
    "            # for top_idx in range(len(top_images)):\n",
    "            #     crop_image = top_images[top_idx][\"crop_image\"]\n",
    "            #     score = matching_score[top_idx]\n",
    "            #     crop_image = cv2.resize(crop_image, (256, 256))\n",
    "            #     cv2.putText(crop_image,f'{score}', (100, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1)\n",
    "            #     image = np.hstack((image, crop_image))\n",
    "            # cv2.imwrite(f\"segment_anything/crop_images/{idx}.jpg\", image)\n",
    "            #---------------------------------------------------\n",
    "            t4 = time.time()\n",
    "            # print(f\"t4-t3: object detection:{1000*(t4-t3)} ms\")\n",
    "            pose0_name = image0_name.replace(\"color\", \"poses_ba\").replace(\"png\", \"txt\")\n",
    "            pose1_name = image1_name.replace(\"color\", \"poses_ba\").replace(\"png\", \"txt\")\n",
    "            pose0 = np.loadtxt(pose0_name)\n",
    "            pose1 = np.loadtxt(pose1_name)\n",
    "            # pose0 = np.vstack((pose0, np.array([[0, 0, 0, 1]])))\n",
    "            # pose1 = np.vstack((pose1, np.array([[0, 0, 0, 1]])))\n",
    "            relative_pose = np.matmul(pose1, inv(pose0))\n",
    "            t = relative_pose[:3, -1].reshape(1, 3)\n",
    "            if \"crop_image\" not in top_images[top_idx]:\n",
    "                continue\n",
    "            max_match_idx = np.argmax(matching_score)\n",
    "            pre_bbox = top_images[max_match_idx][\"bbox\"]\n",
    "            mkpts0 = top_images[max_match_idx][\"mkpts0\"]\n",
    "            mkpts1 = top_images[max_match_idx][\"mkpts1\"]\n",
    "            pre_K = top_images[max_match_idx][\"K\"]\n",
    "\n",
    "            _3d_bbox = np.loadtxt(f\"{os.path.join(ROOT_DIR, label)}/box3d_corners.txt\")\n",
    "            bbox_pts_3d, _ = project_points(_3d_bbox, pose1[:3, :4], K1)\n",
    "            bbox_pts_3d = bbox_pts_3d.astype(np.int32)\n",
    "            x0, y0, w, h = cv2.boundingRect(bbox_pts_3d)\n",
    "            x1, y1 = x0 + w, y0 + h\n",
    "            gt_bbox = np.array([x0, y0, x1, y1])\n",
    "            is_recalled = recall_object(pre_bbox , gt_bbox)\n",
    "            recall_image = recall_image + int(is_recalled>0.5)\n",
    "            ret = estimate_pose(mkpts0, mkpts1, K0, pre_K, 0.5, 0.99)\n",
    "            if ret is not None:\n",
    "                Rot, t, inliers = ret\n",
    "                t_err, R_err = relative_pose_error(relative_pose, Rot, t, ignore_gt_t_thr=0.0)\n",
    "                metrics['R_errs'].append(R_err)\n",
    "                metrics['t_errs'].append(t_err)\n",
    "            else:\n",
    "                metrics['R_errs'].append(90)\n",
    "                metrics['t_errs'].append(90)\n",
    "            metrics[\"identifiers\"].append(pair_name)\n",
    "\n",
    "\n",
    "    print(f\"Acc: {recall_image}/{all_image}\")\n",
    "    import pprint\n",
    "    from src.utils.metrics import (\n",
    "        aggregate_metrics\n",
    "    )\n",
    "    from loguru import logger\n",
    "    val_metrics_4tb = aggregate_metrics(metrics, 5e-4)\n",
    "    val_metrics_4tb[\"AP50\"] = recall_image/all_image\n",
    "    logger.info('\\n' + pprint.pformat(val_metrics_4tb))\n",
    "\n",
    "    res_table.append([f\"{name }\"] + list(val_metrics_4tb.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════════╤════════════╤════════════╤═══════════╤═══════════╤═══════════════╤══════════╕\n",
      "│ Category     │   R:auc@15 │   R:auc@30 │   R:ACC15 │   R:ACC30 │   R:medianErr │   t:auc@15 │   t:auc@30 │   t:ACC15 │   t:ACC30 │   t:medianErr │     AP50 │\n",
      "╞══════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════════╪══════════╡\n",
      "│ toyrobot     │   0.588646 │   0.73373  │  0.807531 │  0.92887  │       4.20232 │   0.514095 │   0.657858 │  0.736402 │  0.832636 │       5.50934 │ 0.962343 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ yellowduck   │   0.520483 │   0.695836 │  0.765079 │  0.936508 │       5.84726 │   0.402714 │   0.567014 │  0.628571 │  0.8      │       9.32537 │ 0.980952 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ sheep        │   0.559614 │   0.705416 │  0.798283 │  0.914163 │       4.94669 │   0.37352  │   0.525094 │  0.587983 │  0.733906 │      10.4936  │ 0.356223 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ fakebanana   │   0.505351 │   0.656369 │  0.72     │  0.89     │       5.66013 │   0.342101 │   0.484352 │  0.563333 │  0.67     │      11.6675  │ 0.97     │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ teabox       │   0.655141 │   0.77299  │  0.852151 │  0.913978 │       2.88576 │   0.535639 │   0.666374 │  0.744624 │  0.833333 │       4.98372 │ 0.978495 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ orange       │   0.444518 │   0.639185 │  0.7      │  0.914706 │       7.7544  │   0.35112  │   0.501454 │  0.564706 │  0.702941 │      11.9118  │ 0.982353 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ greenteapot  │   0.623162 │   0.764025 │  0.840206 │  0.948454 │       3.51475 │   0.491322 │   0.651404 │  0.752577 │  0.829897 │       6.25336 │ 0.974227 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ lecreusetcup │   0.505143 │   0.668327 │  0.7173   │  0.911392 │       6.18569 │   0.343179 │   0.512697 │  0.590717 │  0.751055 │      11.9133  │ 0.831224 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ insta        │   0.265317 │   0.444743 │  0.449136 │  0.763916 │      17.3654  │   0.144216 │   0.229193 │  0.257198 │  0.370441 │      50.3916  │ 0.627639 │\n",
      "├──────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼────────────┼────────────┼───────────┼───────────┼───────────────┼──────────┤\n",
      "│ Avg          │   0.518597 │   0.675624 │  0.738854 │  0.902443 │       6.48471 │   0.388656 │   0.532827 │  0.602901 │  0.724912 │      13.6055  │ 0.851495 │\n",
      "╘══════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════════╧════════════╧════════════╧═══════════╧═══════════╧═══════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "headers = [\"Category\"] + list(val_metrics_4tb.keys())\n",
    "all_data = np.array(res_table)[:, 1:].astype(np.float32)\n",
    "res_table.append([\"Avg\"] + all_data.mean(0).tolist())\n",
    "print(tabulate(res_table, \\\n",
    "    headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
